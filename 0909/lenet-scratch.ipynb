{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import packages\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program\\python\\python37\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# 2. Load data\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28])\n",
      "Shape of y:  torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (conv3): Conv2d(16, 120, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc2): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 3. Create LeNet model\n",
    "# 비교해보면 좋을 코드 1: https://www.youtube.com/watch?v=fcOW-Zyb5Bo\n",
    "class LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 입력 이미지: 채널 1개, 출력 채널 6개, 5x5 정사각 convoluation layer\n",
    "        # 5x5로 하면 에러나서 4x4의 convluation layer\n",
    "        # padding을 바꿔도 됨\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=(4, 4), stride=(1, 1), padding=(0, 0))\n",
    "        # 6개 채널, 16개 채널로 변환\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=4, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(16, 120, kernel_size=4, stride=1) # default padding: 0, default stride: 1\n",
    "\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.fc2 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.conv1(x))\n",
    "        x = F.avg_pool2d(x, kernel_size=(2, 2), stride=(2, 2))\n",
    "        x = F.tanh(self.conv2(x))\n",
    "        x = F.avg_pool2d(x, 2, 2)\n",
    "        x = F.tanh(self.conv3(x))\n",
    "        # x = x.view(-1, 120)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "\n",
    "model = LeNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.1047, -0.0163,  0.1082,  0.2330],\n",
      "          [-0.1238, -0.0171, -0.1885, -0.1406],\n",
      "          [-0.1599, -0.0070, -0.1758, -0.1848],\n",
      "          [-0.1018,  0.0348,  0.1518,  0.1586]]],\n",
      "\n",
      "\n",
      "        [[[-0.1102, -0.2358,  0.0876,  0.0812],\n",
      "          [ 0.0241,  0.1209,  0.0330, -0.0246],\n",
      "          [-0.1026, -0.0312,  0.1346, -0.1026],\n",
      "          [-0.1302, -0.0293, -0.1996, -0.1880]]],\n",
      "\n",
      "\n",
      "        [[[-0.0157, -0.0423,  0.1303, -0.0719],\n",
      "          [-0.2248, -0.0662,  0.1039, -0.2302],\n",
      "          [ 0.0938, -0.1470,  0.1793,  0.0837],\n",
      "          [ 0.0861,  0.0005, -0.1163, -0.1968]]],\n",
      "\n",
      "\n",
      "        [[[-0.0182,  0.0051, -0.0422,  0.0783],\n",
      "          [-0.1988,  0.0866,  0.0651, -0.0758],\n",
      "          [-0.0983,  0.2203, -0.0400,  0.0905],\n",
      "          [-0.1756, -0.2433, -0.1619,  0.1520]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0399, -0.0658,  0.0232,  0.1684],\n",
      "          [ 0.2412, -0.1790, -0.0943, -0.0768],\n",
      "          [-0.1617,  0.2076,  0.0428, -0.1546],\n",
      "          [ 0.0383, -0.1388,  0.2007, -0.0404]]],\n",
      "\n",
      "\n",
      "        [[[-0.2021, -0.0618, -0.1229, -0.0839],\n",
      "          [-0.1635, -0.2424,  0.1868,  0.0441],\n",
      "          [ 0.1196,  0.1315,  0.0416,  0.2393],\n",
      "          [-0.2001,  0.1296,  0.2498,  0.0075]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "model.to(device)\n",
    "print(next(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 25, 25]             102\n",
      "            Conv2d-2             [-1, 16, 9, 9]           1,552\n",
      "            Conv2d-3            [-1, 120, 1, 1]          30,840\n",
      "            Linear-4                   [-1, 84]          10,164\n",
      "            Linear-5                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 43,508\n",
      "Trainable params: 43,508\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.04\n",
      "Params size (MB): 0.17\n",
      "Estimated Total Size (MB): 0.21\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduction 'sum': 데이터의 loss가 너무 작을 때 사용. default average\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # forward\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # backword\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 147.445374  [    0/60000]\n",
      "loss: 106.474106  [ 6400/60000]\n",
      "loss: 101.045715  [12800/60000]\n",
      "loss: 101.037827  [19200/60000]\n",
      "loss: 101.972282  [25600/60000]\n",
      "loss: 101.661926  [32000/60000]\n",
      "loss: 99.746559  [38400/60000]\n",
      "loss: 101.614075  [44800/60000]\n",
      "loss: 98.514923  [51200/60000]\n",
      "loss: 98.295296  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.0%, Avg loss: 97.279851 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 96.669487  [    0/60000]\n",
      "loss: 97.450661  [ 6400/60000]\n",
      "loss: 98.648758  [12800/60000]\n",
      "loss: 97.410240  [19200/60000]\n",
      "loss: 95.558647  [25600/60000]\n",
      "loss: 97.741684  [32000/60000]\n",
      "loss: 94.876572  [38400/60000]\n",
      "loss: 98.802307  [44800/60000]\n",
      "loss: 95.733788  [51200/60000]\n",
      "loss: 97.017715  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.1%, Avg loss: 95.681487 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 95.003609  [    0/60000]\n",
      "loss: 97.154594  [ 6400/60000]\n",
      "loss: 96.684547  [12800/60000]\n",
      "loss: 96.896324  [19200/60000]\n",
      "loss: 94.835556  [25600/60000]\n",
      "loss: 95.567375  [32000/60000]\n",
      "loss: 94.752052  [38400/60000]\n",
      "loss: 96.832382  [44800/60000]\n",
      "loss: 95.348740  [51200/60000]\n",
      "loss: 96.517479  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.8%, Avg loss: 95.280700 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 94.573936  [    0/60000]\n",
      "loss: 96.544411  [ 6400/60000]\n",
      "loss: 96.197891  [12800/60000]\n",
      "loss: 94.642647  [19200/60000]\n",
      "loss: 95.058495  [25600/60000]\n",
      "loss: 96.056183  [32000/60000]\n",
      "loss: 94.725349  [38400/60000]\n",
      "loss: 97.053497  [44800/60000]\n",
      "loss: 94.999649  [51200/60000]\n",
      "loss: 96.553177  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.1%, Avg loss: 94.946716 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 94.563148  [    0/60000]\n",
      "loss: 96.203026  [ 6400/60000]\n",
      "loss: 95.644196  [12800/60000]\n",
      "loss: 94.638229  [19200/60000]\n",
      "loss: 94.506470  [25600/60000]\n",
      "loss: 94.352264  [32000/60000]\n",
      "loss: 94.542542  [38400/60000]\n",
      "loss: 95.055573  [44800/60000]\n",
      "loss: 95.309608  [51200/60000]\n",
      "loss: 96.471901  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.3%, Avg loss: 94.792674 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0925, 0.0847, 0.0968, 0.0952, 0.1049, 0.1065, 0.1041, 0.1026, 0.1059,\n",
      "         0.1068]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-8939fcee8491>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mpredicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Predicted: \"{predicted}\", Actual: \"{actual}\"'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"0\", \"1\", \"2\", \"3\", \"4\"\n",
    "    \"5\", \"6\", \"7\", \"8\", \"9\"\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[1][0], test_data[1][1]\n",
    "with torch.no_grad():\n",
    "    # x.unsqueeze(1): 가상으로 이미지개 여러개 있다고 바꿔줌. 이 경우, 3차원을 4차원으로 바꿔줌.\n",
    "    # 즉 차원을 하나 높여줌\n",
    "    pred = model(x.unsqueeze(1))\n",
    "    print(pred)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d9d6372848>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x.reshape(28, 28), cmap='gist_yarg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
