{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"densenet3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d67bcecae68d4e1897a115b8377779c7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_120fc4d4228f4bcfa8dd692ad91883d9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_89d13dd2b1d54e15b15c48855082aecf","IPY_MODEL_60c01cdfceba4e9d8f1190e0021a76f2","IPY_MODEL_933b5c76c9bb4559b5141061ad404509"]}},"120fc4d4228f4bcfa8dd692ad91883d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"89d13dd2b1d54e15b15c48855082aecf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_40e246f93ff64da2b20735f309ded7de","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_afc039373b8d49b3bc1724dc5ce90580"}},"60c01cdfceba4e9d8f1190e0021a76f2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_eaf58010ecf1461292aaa514ca9a423f","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":170498071,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":170498071,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_311d566ff22043a5b3a77f0c2363b9c1"}},"933b5c76c9bb4559b5141061ad404509":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d98632ecacfc4ba7bb9bc69f83368845","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170499072/? [00:03&lt;00:00, 50409036.28it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c3611d773fa34e1caf2fd966e3292886"}},"40e246f93ff64da2b20735f309ded7de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"afc039373b8d49b3bc1724dc5ce90580":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"eaf58010ecf1461292aaa514ca9a423f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"311d566ff22043a5b3a77f0c2363b9c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d98632ecacfc4ba7bb9bc69f83368845":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c3611d773fa34e1caf2fd966e3292886":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"odDS11PLpK0T"},"source":["## 드라이브 마운트"]},{"cell_type":"code","metadata":{"id":"9U9-RgqJowYe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636550602104,"user_tz":-540,"elapsed":34144,"user":{"displayName":"parrot parrot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16032609930329820738"}},"outputId":"accca643-a5fd-48fb-eb83-1a4aa23104ca"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","basic_path = '/content/drive/MyDrive/test/'"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"W8YFjNQSpQoH"},"source":["## 라이브러리 Import"]},{"cell_type":"code","metadata":{"id":"USjHSSPkpRGf","executionInfo":{"status":"ok","timestamp":1636550626976,"user_tz":-540,"elapsed":24877,"user":{"displayName":"parrot parrot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16032609930329820738"}}},"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch.nn.functional as F\n","from torchsummary import summary"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"bzAYMyFbpWL4","executionInfo":{"status":"ok","timestamp":1636550627270,"user_tz":-540,"elapsed":301,"user":{"displayName":"parrot parrot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16032609930329820738"}}},"source":["def get_device():\n","    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"7FMtneAuqGfZ","executionInfo":{"status":"ok","timestamp":1636550627271,"user_tz":-540,"elapsed":13,"user":{"displayName":"parrot parrot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16032609930329820738"}}},"source":["def get_mean_std(channel, training_dataset):\n","  if channel.lower() is 'rgb':\n","    mean_rgb = [np.mean(x.numpy(), axis=(1, 2)) for x,_ in training_dataset]\n","    std_rgb = [np.std(x.numpy(), axis=(1, 2)) for x,_ in training_dataset]\n","\n","    mean_r = np.mean([m[0] for m in mean_rgb])\n","    mean_g = np.mean([m[1] for m in mean_rgb])\n","    mean_b = np.mean([m[2] for m in mean_rgb])\n","\n","    std_r = np.mean([s[0] for s in std_rgb])\n","    std_g = np.mean([s[1] for s in std_rgb])\n","    std_b = np.mean([s[2] for s in std_rgb])\n","    return [mean_r, mean_g, mean_b], [std_r, std_g, std_b]\n","  else:\n","    mean = [np.mean(x.numpy(), axis=(1, 2)) for x,_ in training_dataset]\n","    std = [np.std(x.numpy(), axis=(1, 2)) for x,_ in training_dataset]\n","\n","    return np.mean([m[0] for m in mean]), np.mean([s[0] for s in std])"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_bie4LzOpYQR"},"source":["## DenseNet 구현"]},{"cell_type":"markdown","metadata":{"id":"dVrehi9GNBK3"},"source":["### BottleneckLayer 구현"]},{"cell_type":"code","metadata":{"id":"JYa_XFYCNAnM","executionInfo":{"status":"ok","timestamp":1636550627272,"user_tz":-540,"elapsed":13,"user":{"displayName":"parrot parrot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16032609930329820738"}}},"source":["class BottleneckLayer(nn.Module):\n","  def __init__(self, in_channel, base_out_channel, dropout_rate=0.2):\n","    super(BottleneckLayer, self).__init__()\n","    self.norm1 = nn.BatchNorm2d(in_channel)\n","    self.relu = nn.ReLU(inplace=True)\n","    self.conv1 = nn.Conv2d(in_channel, base_out_channel*4, kernel_size=1, stride=1, padding=0, bias=False)\n","    self.norm2 = nn.BatchNorm2d(base_out_channel*4)\n","    self.conv2 = nn.Conv2d(base_out_channel*4, base_out_channel, kernel_size=3, stride=1, padding=1, bias=False)\n","\n","    self.dropout_rate =dropout_rate\n","  \n","  def forward(self, x):\n","      y = self.conv1(self.relu(self.norm1(x)))\n","      if self.dropout_rate > 0:\n","        y = F.dropout(y, p=self.dropout_rate, training=self.training)\n","      y = self.conv2(self.relu(self.norm2(y)))\n","      if self.dropout_rate > 0:\n","        y = F.dropout(y, p=self.dropout_rate, training=self.training)\n","      return torch.cat([x, y], 1)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4H-txPQtpaNJ"},"source":["### DenseBlock 구현"]},{"cell_type":"code","metadata":{"id":"2mrOeUt1pdlW","executionInfo":{"status":"ok","timestamp":1636550627273,"user_tz":-540,"elapsed":13,"user":{"displayName":"parrot parrot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16032609930329820738"}}},"source":["class DenseBlock(nn.Sequential):\n","    def __init__(self, in_channel, growth_rate, number_of_layer):\n","        super(DenseBlock, self).__init__()\n","        current_in_channel = in_channel\n","        for number in range(number_of_layer):\n","            # DenseBlock 내부에 쌓이는 Layer 구성\n","            current_in_channel = in_channel + (growth_rate * number)\n","            current_layer_name = 'DENSEBLOCK_LAYER_' + str(number)\n","            self.add_module(current_layer_name, BottleneckLayer(current_in_channel, growth_rate))"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ksCbGVBdpfS3"},"source":["### Transition Layer 구현"]},{"cell_type":"code","metadata":{"id":"MHoHeHQ4phkl","executionInfo":{"status":"ok","timestamp":1636550627273,"user_tz":-540,"elapsed":13,"user":{"displayName":"parrot parrot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16032609930329820738"}}},"source":["class TransitionLayer(nn.Sequential):\n","    def __init__(self, in_channel, reduction):\n","        super(TransitionLayer, self).__init__()\n","        self.add_module('BATCH_NORM', nn.BatchNorm2d(in_channel))\n","        self.add_module('RELU', nn.ReLU(inplace=True))\n","        self.add_module('CONV_1x1', nn.Conv2d(in_channel, int(in_channel*reduction), kernel_size=1, stride=1, bias=False))\n","        self.add_module('AVG_POOL_2x2', nn.AvgPool2d(kernel_size=2, stride=2))"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jcUkzqnLpk7u"},"source":["### 모델 구현"]},{"cell_type":"code","metadata":{"id":"dVuP6D8tpjdX","executionInfo":{"status":"ok","timestamp":1636550627274,"user_tz":-540,"elapsed":13,"user":{"displayName":"parrot parrot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16032609930329820738"}}},"source":["class MyDenseNet(nn.Module):\n","    def __init__(self, size_of_channel, growth_rate, number_of_class, interation, reduction = 0.5):\n","        super(MyDenseNet, self).__init__()\n","        self.input = nn.Conv2d(size_of_channel, growth_rate*2, kernel_size=7, stride=2, padding=3, bias=True)\n","        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        \n","        # 1) e.g., Type is DenseNet-121: input → dense → transition: 64 → 256 → 128\n","        in_channel = growth_rate*2\n","        self.dense_block_1 = DenseBlock(in_channel, growth_rate, number_of_layer=interation[0])\n","        out_channel = in_channel + (growth_rate*interation[0])\n","        self.transition_layer_1 = TransitionLayer(out_channel, reduction)\n","        \n","        # 2) e.g., Type is DenseNet-121: input → dense → transition: 128 → 512 → 256\n","        in_channel = int(out_channel*reduction)\n","        self.dense_block_2 = DenseBlock(in_channel, growth_rate, number_of_layer=interation[1])\n","        out_channel = in_channel + (growth_rate*interation[1])\n","        self.transition_layer_2 = TransitionLayer(out_channel, reduction)\n","        \n","        # 3) e.g., Type is DenseNet-121: input → dense → transition: 256 → 1024 → 512\n","        in_channel = int(out_channel*reduction)\n","        self.dense_block_3 = DenseBlock(in_channel, growth_rate, number_of_layer=interation[2])\n","        out_channel = in_channel + (growth_rate*interation[2])\n","        self.transition_layer_3 = TransitionLayer(out_channel, reduction)\n","        \n","        # 4) e.g., Type is DenseNet-121: input → dense → transition: 512 → 1024\n","        in_channel = int(out_channel*reduction)\n","        self.dense_block_4 = DenseBlock(in_channel, growth_rate, number_of_layer=interation[3])\n","        out_channel = in_channel + (growth_rate*interation[3])\n","        \n","        self.fc_layer = nn.Linear(out_channel, number_of_class)\n","        self.__initialize_weights__()\n","\n","    def __initialize_weights__(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, 0, 0.01)\n","                nn.init.constant_(m.bias, 0)\n","    \n","    def forward(self, x):\n","        input_feature_map = self.max_pool(self.input(x))\n","        \n","        dense_block_output_1 = self.dense_block_1(input_feature_map)\n","        transition_layer_output_1 = self.transition_layer_1(dense_block_output_1)\n","        \n","        dense_block_output_2 = self.dense_block_2(transition_layer_output_1)\n","        transition_layer_output_2 = self.transition_layer_2(dense_block_output_2)\n","        \n","        dense_block_output_3 = self.dense_block_3(transition_layer_output_2)\n","        transition_layer_output_3 = self.transition_layer_3(dense_block_output_3)\n","        \n","        dense_block_output_4 = self.dense_block_4(transition_layer_output_3)\n","        \n","        avg_pool_output = nn.functional.adaptive_avg_pool2d(dense_block_output_4, (1, 1))\n","        flatten = avg_pool_output.view(avg_pool_output.size(0), -1)\n","        \n","        output = self.fc_layer(flatten)\n","        return output"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"j5-WsqPzpqRZ","executionInfo":{"status":"ok","timestamp":1636550627274,"user_tz":-540,"elapsed":12,"user":{"displayName":"parrot parrot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16032609930329820738"}}},"source":["GROWTH_RATE = 32\n","ITERATIONS_PER_DEPTH = {\n","    'DenseNet-121': [6, 12, 24, 16],\n","    'DenseNet-169': [6, 12, 32, 32],\n","    'DenseNet-201': [6, 12, 48, 32],\n","    'DenseNet-264': [6, 12, 64, 48],\n","};"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YbE3Qnfcpq7x"},"source":["## Train / Test 코드"]},{"cell_type":"code","metadata":{"id":"UKhmPSgfptK6","executionInfo":{"status":"ok","timestamp":1636550627275,"user_tz":-540,"elapsed":12,"user":{"displayName":"parrot parrot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16032609930329820738"}}},"source":["def train(device, dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","    model.train()\n","    \n","    for batch, (X, y) in enumerate(dataloader):\n","      X, y = X.to(device), y.to(device)\n","      \n","      pred = model(X.cuda())\n","      loss = loss_fn(pred, y)\n","      \n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","      \n","      print('.', end='')\n","      if batch % 100 == 0:\n","          print()\n","          loss, current = loss.item(), batch*len(X)\n","          print(f'loss: {loss:>7f}   [{current:>5d}/{size:>5d}]')"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"vNjd_TYdpwzA","executionInfo":{"status":"ok","timestamp":1636550627276,"user_tz":-540,"elapsed":12,"user":{"displayName":"parrot parrot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16032609930329820738"}}},"source":["def test(device, dataloader, model, loss_fn):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    model.eval()\n","    \n","    test_loss, correct = 0, 0\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            X, y = X.to(device), y.to(device)\n","            \n","            pred = model(X)\n","            test_loss += loss_fn(pred, y).item()\n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","    test_loss /= num_batches\n","    correct /= size\n","    print(f'Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loass: {test_loss:>8f}\\n')"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"KeY_RfgkpzIH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636550627572,"user_tz":-540,"elapsed":308,"user":{"displayName":"parrot parrot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16032609930329820738"}},"outputId":"b11a865a-fbc5-40d8-83fa-c0b1c77d90b1"},"source":["device = get_device()\n","print('Device:', device)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n"]}]},{"cell_type":"markdown","metadata":{"id":"BJJ2iDUid3mV"},"source":["## Model - CIFAR-10"]},{"cell_type":"code","metadata":{"id":"_adiXo5ldvyD","colab":{"base_uri":"https://localhost:8080/","height":102,"referenced_widgets":["d67bcecae68d4e1897a115b8377779c7","120fc4d4228f4bcfa8dd692ad91883d9","89d13dd2b1d54e15b15c48855082aecf","60c01cdfceba4e9d8f1190e0021a76f2","933b5c76c9bb4559b5141061ad404509","40e246f93ff64da2b20735f309ded7de","afc039373b8d49b3bc1724dc5ce90580","eaf58010ecf1461292aaa514ca9a423f","311d566ff22043a5b3a77f0c2363b9c1","d98632ecacfc4ba7bb9bc69f83368845","c3611d773fa34e1caf2fd966e3292886"]},"executionInfo":{"status":"ok","timestamp":1636550663223,"user_tz":-540,"elapsed":35663,"user":{"displayName":"parrot parrot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16032609930329820738"}},"outputId":"04b0b074-5e9c-4109-ed63-f9b04a55af82"},"source":["training_dataset_cifar10 = datasets.CIFAR10(\n","    root=basic_path + '/data', train=True, download=True, transform=transforms.ToTensor(),\n",")\n","test_dataset_cifar10 = datasets.CIFAR10(\n","    root=basic_path + '/data', train=False, download=True, transform=transforms.ToTensor(),\n",")\n","\n","rgb_mean, rgb_std = get_mean_std('rgb', training_dataset_cifar10)\n","rgb_transform = transforms.Compose([\n","  transforms.ToTensor(),\n","  transforms.RandomHorizontalFlip(),\n","  transforms.Resize(224),\n","  transforms.Normalize(mean=[rgb_mean], std=[rgb_std])\n","])\n","training_dataset_cifar10.transform = rgb_transform\n","test_dataset_cifar10.transform = rgb_transform"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/drive/MyDrive/test//data/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d67bcecae68d4e1897a115b8377779c7","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting /content/drive/MyDrive/test//data/cifar-10-python.tar.gz to /content/drive/MyDrive/test//data\n","Files already downloaded and verified\n"]}]},{"cell_type":"code","metadata":{"id":"q2ovm0SIdv1z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636550663604,"user_tz":-540,"elapsed":396,"user":{"displayName":"parrot parrot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16032609930329820738"}},"outputId":"f36d3ab1-9a45-4bb1-a4d1-a7a0b8adcf38"},"source":["training_dataloader_cifar10 = DataLoader(training_dataset_cifar10, batch_size=64)\n","test_dataloader_cifar10 = DataLoader(test_dataset_cifar10, batch_size=64)\n","\n","for X, y in test_dataloader_cifar10:\n","  print('Shape of X [N, C, H, W]:', X.shape)\n","  print('Shape of y:', y.shape, y.dtype)\n","  break"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of X [N, C, H, W]: torch.Size([64, 3, 224, 224])\n","Shape of y: torch.Size([64]) torch.int64\n"]}]},{"cell_type":"code","metadata":{"id":"ZyEWbC3UeEl6","executionInfo":{"status":"ok","timestamp":1636550678231,"user_tz":-540,"elapsed":14630,"user":{"displayName":"parrot parrot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16032609930329820738"}}},"source":["number_of_cifar10_classes = 10\n","model_cifar10 = MyDenseNet(3, GROWTH_RATE, number_of_cifar10_classes, ITERATIONS_PER_DEPTH['DenseNet-121'])\n","model_cifar10 = model_cifar10.to(device)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"UW_xTqZ4dv5e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636550678845,"user_tz":-540,"elapsed":629,"user":{"displayName":"parrot parrot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16032609930329820738"}},"outputId":"022c5bbb-d6f2-434b-e930-b86ecb49ac35"},"source":["summary(model_cifar10, input_size=(3, 224, 224))"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"]},{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 112, 112]           9,472\n","         MaxPool2d-2           [-1, 64, 56, 56]               0\n","       BatchNorm2d-3           [-1, 64, 56, 56]             128\n","              ReLU-4           [-1, 64, 56, 56]               0\n","            Conv2d-5          [-1, 128, 56, 56]           8,192\n","       BatchNorm2d-6          [-1, 128, 56, 56]             256\n","              ReLU-7          [-1, 128, 56, 56]               0\n","            Conv2d-8           [-1, 32, 56, 56]          36,864\n","   BottleneckLayer-9           [-1, 96, 56, 56]               0\n","      BatchNorm2d-10           [-1, 96, 56, 56]             192\n","             ReLU-11           [-1, 96, 56, 56]               0\n","           Conv2d-12          [-1, 128, 56, 56]          12,288\n","      BatchNorm2d-13          [-1, 128, 56, 56]             256\n","             ReLU-14          [-1, 128, 56, 56]               0\n","           Conv2d-15           [-1, 32, 56, 56]          36,864\n","  BottleneckLayer-16          [-1, 128, 56, 56]               0\n","      BatchNorm2d-17          [-1, 128, 56, 56]             256\n","             ReLU-18          [-1, 128, 56, 56]               0\n","           Conv2d-19          [-1, 128, 56, 56]          16,384\n","      BatchNorm2d-20          [-1, 128, 56, 56]             256\n","             ReLU-21          [-1, 128, 56, 56]               0\n","           Conv2d-22           [-1, 32, 56, 56]          36,864\n","  BottleneckLayer-23          [-1, 160, 56, 56]               0\n","      BatchNorm2d-24          [-1, 160, 56, 56]             320\n","             ReLU-25          [-1, 160, 56, 56]               0\n","           Conv2d-26          [-1, 128, 56, 56]          20,480\n","      BatchNorm2d-27          [-1, 128, 56, 56]             256\n","             ReLU-28          [-1, 128, 56, 56]               0\n","           Conv2d-29           [-1, 32, 56, 56]          36,864\n","  BottleneckLayer-30          [-1, 192, 56, 56]               0\n","      BatchNorm2d-31          [-1, 192, 56, 56]             384\n","             ReLU-32          [-1, 192, 56, 56]               0\n","           Conv2d-33          [-1, 128, 56, 56]          24,576\n","      BatchNorm2d-34          [-1, 128, 56, 56]             256\n","             ReLU-35          [-1, 128, 56, 56]               0\n","           Conv2d-36           [-1, 32, 56, 56]          36,864\n","  BottleneckLayer-37          [-1, 224, 56, 56]               0\n","      BatchNorm2d-38          [-1, 224, 56, 56]             448\n","             ReLU-39          [-1, 224, 56, 56]               0\n","           Conv2d-40          [-1, 128, 56, 56]          28,672\n","      BatchNorm2d-41          [-1, 128, 56, 56]             256\n","             ReLU-42          [-1, 128, 56, 56]               0\n","           Conv2d-43           [-1, 32, 56, 56]          36,864\n","  BottleneckLayer-44          [-1, 256, 56, 56]               0\n","      BatchNorm2d-45          [-1, 256, 56, 56]             512\n","             ReLU-46          [-1, 256, 56, 56]               0\n","           Conv2d-47          [-1, 128, 56, 56]          32,768\n","        AvgPool2d-48          [-1, 128, 28, 28]               0\n","      BatchNorm2d-49          [-1, 128, 28, 28]             256\n","             ReLU-50          [-1, 128, 28, 28]               0\n","           Conv2d-51          [-1, 128, 28, 28]          16,384\n","      BatchNorm2d-52          [-1, 128, 28, 28]             256\n","             ReLU-53          [-1, 128, 28, 28]               0\n","           Conv2d-54           [-1, 32, 28, 28]          36,864\n","  BottleneckLayer-55          [-1, 160, 28, 28]               0\n","      BatchNorm2d-56          [-1, 160, 28, 28]             320\n","             ReLU-57          [-1, 160, 28, 28]               0\n","           Conv2d-58          [-1, 128, 28, 28]          20,480\n","      BatchNorm2d-59          [-1, 128, 28, 28]             256\n","             ReLU-60          [-1, 128, 28, 28]               0\n","           Conv2d-61           [-1, 32, 28, 28]          36,864\n","  BottleneckLayer-62          [-1, 192, 28, 28]               0\n","      BatchNorm2d-63          [-1, 192, 28, 28]             384\n","             ReLU-64          [-1, 192, 28, 28]               0\n","           Conv2d-65          [-1, 128, 28, 28]          24,576\n","      BatchNorm2d-66          [-1, 128, 28, 28]             256\n","             ReLU-67          [-1, 128, 28, 28]               0\n","           Conv2d-68           [-1, 32, 28, 28]          36,864\n","  BottleneckLayer-69          [-1, 224, 28, 28]               0\n","      BatchNorm2d-70          [-1, 224, 28, 28]             448\n","             ReLU-71          [-1, 224, 28, 28]               0\n","           Conv2d-72          [-1, 128, 28, 28]          28,672\n","      BatchNorm2d-73          [-1, 128, 28, 28]             256\n","             ReLU-74          [-1, 128, 28, 28]               0\n","           Conv2d-75           [-1, 32, 28, 28]          36,864\n","  BottleneckLayer-76          [-1, 256, 28, 28]               0\n","      BatchNorm2d-77          [-1, 256, 28, 28]             512\n","             ReLU-78          [-1, 256, 28, 28]               0\n","           Conv2d-79          [-1, 128, 28, 28]          32,768\n","      BatchNorm2d-80          [-1, 128, 28, 28]             256\n","             ReLU-81          [-1, 128, 28, 28]               0\n","           Conv2d-82           [-1, 32, 28, 28]          36,864\n","  BottleneckLayer-83          [-1, 288, 28, 28]               0\n","      BatchNorm2d-84          [-1, 288, 28, 28]             576\n","             ReLU-85          [-1, 288, 28, 28]               0\n","           Conv2d-86          [-1, 128, 28, 28]          36,864\n","      BatchNorm2d-87          [-1, 128, 28, 28]             256\n","             ReLU-88          [-1, 128, 28, 28]               0\n","           Conv2d-89           [-1, 32, 28, 28]          36,864\n","  BottleneckLayer-90          [-1, 320, 28, 28]               0\n","      BatchNorm2d-91          [-1, 320, 28, 28]             640\n","             ReLU-92          [-1, 320, 28, 28]               0\n","           Conv2d-93          [-1, 128, 28, 28]          40,960\n","      BatchNorm2d-94          [-1, 128, 28, 28]             256\n","             ReLU-95          [-1, 128, 28, 28]               0\n","           Conv2d-96           [-1, 32, 28, 28]          36,864\n","  BottleneckLayer-97          [-1, 352, 28, 28]               0\n","      BatchNorm2d-98          [-1, 352, 28, 28]             704\n","             ReLU-99          [-1, 352, 28, 28]               0\n","          Conv2d-100          [-1, 128, 28, 28]          45,056\n","     BatchNorm2d-101          [-1, 128, 28, 28]             256\n","            ReLU-102          [-1, 128, 28, 28]               0\n","          Conv2d-103           [-1, 32, 28, 28]          36,864\n"," BottleneckLayer-104          [-1, 384, 28, 28]               0\n","     BatchNorm2d-105          [-1, 384, 28, 28]             768\n","            ReLU-106          [-1, 384, 28, 28]               0\n","          Conv2d-107          [-1, 128, 28, 28]          49,152\n","     BatchNorm2d-108          [-1, 128, 28, 28]             256\n","            ReLU-109          [-1, 128, 28, 28]               0\n","          Conv2d-110           [-1, 32, 28, 28]          36,864\n"," BottleneckLayer-111          [-1, 416, 28, 28]               0\n","     BatchNorm2d-112          [-1, 416, 28, 28]             832\n","            ReLU-113          [-1, 416, 28, 28]               0\n","          Conv2d-114          [-1, 128, 28, 28]          53,248\n","     BatchNorm2d-115          [-1, 128, 28, 28]             256\n","            ReLU-116          [-1, 128, 28, 28]               0\n","          Conv2d-117           [-1, 32, 28, 28]          36,864\n"," BottleneckLayer-118          [-1, 448, 28, 28]               0\n","     BatchNorm2d-119          [-1, 448, 28, 28]             896\n","            ReLU-120          [-1, 448, 28, 28]               0\n","          Conv2d-121          [-1, 128, 28, 28]          57,344\n","     BatchNorm2d-122          [-1, 128, 28, 28]             256\n","            ReLU-123          [-1, 128, 28, 28]               0\n","          Conv2d-124           [-1, 32, 28, 28]          36,864\n"," BottleneckLayer-125          [-1, 480, 28, 28]               0\n","     BatchNorm2d-126          [-1, 480, 28, 28]             960\n","            ReLU-127          [-1, 480, 28, 28]               0\n","          Conv2d-128          [-1, 128, 28, 28]          61,440\n","     BatchNorm2d-129          [-1, 128, 28, 28]             256\n","            ReLU-130          [-1, 128, 28, 28]               0\n","          Conv2d-131           [-1, 32, 28, 28]          36,864\n"," BottleneckLayer-132          [-1, 512, 28, 28]               0\n","     BatchNorm2d-133          [-1, 512, 28, 28]           1,024\n","            ReLU-134          [-1, 512, 28, 28]               0\n","          Conv2d-135          [-1, 256, 28, 28]         131,072\n","       AvgPool2d-136          [-1, 256, 14, 14]               0\n","     BatchNorm2d-137          [-1, 256, 14, 14]             512\n","            ReLU-138          [-1, 256, 14, 14]               0\n","          Conv2d-139          [-1, 128, 14, 14]          32,768\n","     BatchNorm2d-140          [-1, 128, 14, 14]             256\n","            ReLU-141          [-1, 128, 14, 14]               0\n","          Conv2d-142           [-1, 32, 14, 14]          36,864\n"," BottleneckLayer-143          [-1, 288, 14, 14]               0\n","     BatchNorm2d-144          [-1, 288, 14, 14]             576\n","            ReLU-145          [-1, 288, 14, 14]               0\n","          Conv2d-146          [-1, 128, 14, 14]          36,864\n","     BatchNorm2d-147          [-1, 128, 14, 14]             256\n","            ReLU-148          [-1, 128, 14, 14]               0\n","          Conv2d-149           [-1, 32, 14, 14]          36,864\n"," BottleneckLayer-150          [-1, 320, 14, 14]               0\n","     BatchNorm2d-151          [-1, 320, 14, 14]             640\n","            ReLU-152          [-1, 320, 14, 14]               0\n","          Conv2d-153          [-1, 128, 14, 14]          40,960\n","     BatchNorm2d-154          [-1, 128, 14, 14]             256\n","            ReLU-155          [-1, 128, 14, 14]               0\n","          Conv2d-156           [-1, 32, 14, 14]          36,864\n"," BottleneckLayer-157          [-1, 352, 14, 14]               0\n","     BatchNorm2d-158          [-1, 352, 14, 14]             704\n","            ReLU-159          [-1, 352, 14, 14]               0\n","          Conv2d-160          [-1, 128, 14, 14]          45,056\n","     BatchNorm2d-161          [-1, 128, 14, 14]             256\n","            ReLU-162          [-1, 128, 14, 14]               0\n","          Conv2d-163           [-1, 32, 14, 14]          36,864\n"," BottleneckLayer-164          [-1, 384, 14, 14]               0\n","     BatchNorm2d-165          [-1, 384, 14, 14]             768\n","            ReLU-166          [-1, 384, 14, 14]               0\n","          Conv2d-167          [-1, 128, 14, 14]          49,152\n","     BatchNorm2d-168          [-1, 128, 14, 14]             256\n","            ReLU-169          [-1, 128, 14, 14]               0\n","          Conv2d-170           [-1, 32, 14, 14]          36,864\n"," BottleneckLayer-171          [-1, 416, 14, 14]               0\n","     BatchNorm2d-172          [-1, 416, 14, 14]             832\n","            ReLU-173          [-1, 416, 14, 14]               0\n","          Conv2d-174          [-1, 128, 14, 14]          53,248\n","     BatchNorm2d-175          [-1, 128, 14, 14]             256\n","            ReLU-176          [-1, 128, 14, 14]               0\n","          Conv2d-177           [-1, 32, 14, 14]          36,864\n"," BottleneckLayer-178          [-1, 448, 14, 14]               0\n","     BatchNorm2d-179          [-1, 448, 14, 14]             896\n","            ReLU-180          [-1, 448, 14, 14]               0\n","          Conv2d-181          [-1, 128, 14, 14]          57,344\n","     BatchNorm2d-182          [-1, 128, 14, 14]             256\n","            ReLU-183          [-1, 128, 14, 14]               0\n","          Conv2d-184           [-1, 32, 14, 14]          36,864\n"," BottleneckLayer-185          [-1, 480, 14, 14]               0\n","     BatchNorm2d-186          [-1, 480, 14, 14]             960\n","            ReLU-187          [-1, 480, 14, 14]               0\n","          Conv2d-188          [-1, 128, 14, 14]          61,440\n","     BatchNorm2d-189          [-1, 128, 14, 14]             256\n","            ReLU-190          [-1, 128, 14, 14]               0\n","          Conv2d-191           [-1, 32, 14, 14]          36,864\n"," BottleneckLayer-192          [-1, 512, 14, 14]               0\n","     BatchNorm2d-193          [-1, 512, 14, 14]           1,024\n","            ReLU-194          [-1, 512, 14, 14]               0\n","          Conv2d-195          [-1, 128, 14, 14]          65,536\n","     BatchNorm2d-196          [-1, 128, 14, 14]             256\n","            ReLU-197          [-1, 128, 14, 14]               0\n","          Conv2d-198           [-1, 32, 14, 14]          36,864\n"," BottleneckLayer-199          [-1, 544, 14, 14]               0\n","     BatchNorm2d-200          [-1, 544, 14, 14]           1,088\n","            ReLU-201          [-1, 544, 14, 14]               0\n","          Conv2d-202          [-1, 128, 14, 14]          69,632\n","     BatchNorm2d-203          [-1, 128, 14, 14]             256\n","            ReLU-204          [-1, 128, 14, 14]               0\n","          Conv2d-205           [-1, 32, 14, 14]          36,864\n"," BottleneckLayer-206          [-1, 576, 14, 14]               0\n","     BatchNorm2d-207          [-1, 576, 14, 14]           1,152\n","            ReLU-208          [-1, 576, 14, 14]               0\n","          Conv2d-209          [-1, 128, 14, 14]          73,728\n","     BatchNorm2d-210          [-1, 128, 14, 14]             256\n","            ReLU-211          [-1, 128, 14, 14]               0\n","          Conv2d-212           [-1, 32, 14, 14]          36,864\n"," BottleneckLayer-213          [-1, 608, 14, 14]               0\n","     BatchNorm2d-214          [-1, 608, 14, 14]           1,216\n","            ReLU-215          [-1, 608, 14, 14]               0\n","          Conv2d-216          [-1, 128, 14, 14]          77,824\n","     BatchNorm2d-217          [-1, 128, 14, 14]             256\n","            ReLU-218          [-1, 128, 14, 14]               0\n","          Conv2d-219           [-1, 32, 14, 14]          36,864\n"," BottleneckLayer-220          [-1, 640, 14, 14]               0\n","     BatchNorm2d-221          [-1, 640, 14, 14]           1,280\n","            ReLU-222          [-1, 640, 14, 14]               0\n","          Conv2d-223          [-1, 128, 14, 14]          81,920\n","     BatchNorm2d-224          [-1, 128, 14, 14]             256\n","            ReLU-225          [-1, 128, 14, 14]               0\n","          Conv2d-226           [-1, 32, 14, 14]          36,864\n"," BottleneckLayer-227          [-1, 672, 14, 14]               0\n","     BatchNorm2d-228          [-1, 672, 14, 14]           1,344\n","            ReLU-229          [-1, 672, 14, 14]               0\n","          Conv2d-230          [-1, 128, 14, 14]          86,016\n","     BatchNorm2d-231          [-1, 128, 14, 14]             256\n","            ReLU-232          [-1, 128, 14, 14]               0\n","          Conv2d-233           [-1, 32, 14, 14]          36,864\n"," BottleneckLayer-234          [-1, 704, 14, 14]               0\n","     BatchNorm2d-235          [-1, 704, 14, 14]           1,408\n","            ReLU-236          [-1, 704, 14, 14]               0\n","          Conv2d-237          [-1, 128, 14, 14]          90,112\n","     BatchNorm2d-238          [-1, 128, 14, 14]             256\n","            ReLU-239          [-1, 128, 14, 14]               0\n","          Conv2d-240           [-1, 32, 14, 14]          36,864\n"," BottleneckLayer-241          [-1, 736, 14, 14]               0\n","     BatchNorm2d-242          [-1, 736, 14, 14]           1,472\n","            ReLU-243          [-1, 736, 14, 14]               0\n","          Conv2d-244          [-1, 128, 14, 14]          94,208\n","     BatchNorm2d-245          [-1, 128, 14, 14]             256\n","            ReLU-246          [-1, 128, 14, 14]               0\n","          Conv2d-247           [-1, 32, 14, 14]          36,864\n"," BottleneckLayer-248          [-1, 768, 14, 14]               0\n","     BatchNorm2d-249          [-1, 768, 14, 14]           1,536\n","            ReLU-250          [-1, 768, 14, 14]               0\n","          Conv2d-251          [-1, 128, 14, 14]          98,304\n","     BatchNorm2d-252          [-1, 128, 14, 14]             256\n","            ReLU-253          [-1, 128, 14, 14]               0\n","          Conv2d-254           [-1, 32, 14, 14]          36,864\n"," BottleneckLayer-255          [-1, 800, 14, 14]               0\n","     BatchNorm2d-256          [-1, 800, 14, 14]           1,600\n","            ReLU-257          [-1, 800, 14, 14]               0\n","          Conv2d-258          [-1, 128, 14, 14]         102,400\n","     BatchNorm2d-259          [-1, 128, 14, 14]             256\n","            ReLU-260          [-1, 128, 14, 14]               0\n","          Conv2d-261           [-1, 32, 14, 14]          36,864\n"," BottleneckLayer-262          [-1, 832, 14, 14]               0\n","     BatchNorm2d-263          [-1, 832, 14, 14]           1,664\n","            ReLU-264          [-1, 832, 14, 14]               0\n","          Conv2d-265          [-1, 128, 14, 14]         106,496\n","     BatchNorm2d-266          [-1, 128, 14, 14]             256\n","            ReLU-267          [-1, 128, 14, 14]               0\n","          Conv2d-268           [-1, 32, 14, 14]          36,864\n"," BottleneckLayer-269          [-1, 864, 14, 14]               0\n","     BatchNorm2d-270          [-1, 864, 14, 14]           1,728\n","            ReLU-271          [-1, 864, 14, 14]               0\n","          Conv2d-272          [-1, 128, 14, 14]         110,592\n","     BatchNorm2d-273          [-1, 128, 14, 14]             256\n","            ReLU-274          [-1, 128, 14, 14]               0\n","          Conv2d-275           [-1, 32, 14, 14]          36,864\n"," BottleneckLayer-276          [-1, 896, 14, 14]               0\n","     BatchNorm2d-277          [-1, 896, 14, 14]           1,792\n","            ReLU-278          [-1, 896, 14, 14]               0\n","          Conv2d-279          [-1, 128, 14, 14]         114,688\n","     BatchNorm2d-280          [-1, 128, 14, 14]             256\n","            ReLU-281          [-1, 128, 14, 14]               0\n","          Conv2d-282           [-1, 32, 14, 14]          36,864\n"," BottleneckLayer-283          [-1, 928, 14, 14]               0\n","     BatchNorm2d-284          [-1, 928, 14, 14]           1,856\n","            ReLU-285          [-1, 928, 14, 14]               0\n","          Conv2d-286          [-1, 128, 14, 14]         118,784\n","     BatchNorm2d-287          [-1, 128, 14, 14]             256\n","            ReLU-288          [-1, 128, 14, 14]               0\n","          Conv2d-289           [-1, 32, 14, 14]          36,864\n"," BottleneckLayer-290          [-1, 960, 14, 14]               0\n","     BatchNorm2d-291          [-1, 960, 14, 14]           1,920\n","            ReLU-292          [-1, 960, 14, 14]               0\n","          Conv2d-293          [-1, 128, 14, 14]         122,880\n","     BatchNorm2d-294          [-1, 128, 14, 14]             256\n","            ReLU-295          [-1, 128, 14, 14]               0\n","          Conv2d-296           [-1, 32, 14, 14]          36,864\n"," BottleneckLayer-297          [-1, 992, 14, 14]               0\n","     BatchNorm2d-298          [-1, 992, 14, 14]           1,984\n","            ReLU-299          [-1, 992, 14, 14]               0\n","          Conv2d-300          [-1, 128, 14, 14]         126,976\n","     BatchNorm2d-301          [-1, 128, 14, 14]             256\n","            ReLU-302          [-1, 128, 14, 14]               0\n","          Conv2d-303           [-1, 32, 14, 14]          36,864\n"," BottleneckLayer-304         [-1, 1024, 14, 14]               0\n","     BatchNorm2d-305         [-1, 1024, 14, 14]           2,048\n","            ReLU-306         [-1, 1024, 14, 14]               0\n","          Conv2d-307          [-1, 512, 14, 14]         524,288\n","       AvgPool2d-308            [-1, 512, 7, 7]               0\n","     BatchNorm2d-309            [-1, 512, 7, 7]           1,024\n","            ReLU-310            [-1, 512, 7, 7]               0\n","          Conv2d-311            [-1, 128, 7, 7]          65,536\n","     BatchNorm2d-312            [-1, 128, 7, 7]             256\n","            ReLU-313            [-1, 128, 7, 7]               0\n","          Conv2d-314             [-1, 32, 7, 7]          36,864\n"," BottleneckLayer-315            [-1, 544, 7, 7]               0\n","     BatchNorm2d-316            [-1, 544, 7, 7]           1,088\n","            ReLU-317            [-1, 544, 7, 7]               0\n","          Conv2d-318            [-1, 128, 7, 7]          69,632\n","     BatchNorm2d-319            [-1, 128, 7, 7]             256\n","            ReLU-320            [-1, 128, 7, 7]               0\n","          Conv2d-321             [-1, 32, 7, 7]          36,864\n"," BottleneckLayer-322            [-1, 576, 7, 7]               0\n","     BatchNorm2d-323            [-1, 576, 7, 7]           1,152\n","            ReLU-324            [-1, 576, 7, 7]               0\n","          Conv2d-325            [-1, 128, 7, 7]          73,728\n","     BatchNorm2d-326            [-1, 128, 7, 7]             256\n","            ReLU-327            [-1, 128, 7, 7]               0\n","          Conv2d-328             [-1, 32, 7, 7]          36,864\n"," BottleneckLayer-329            [-1, 608, 7, 7]               0\n","     BatchNorm2d-330            [-1, 608, 7, 7]           1,216\n","            ReLU-331            [-1, 608, 7, 7]               0\n","          Conv2d-332            [-1, 128, 7, 7]          77,824\n","     BatchNorm2d-333            [-1, 128, 7, 7]             256\n","            ReLU-334            [-1, 128, 7, 7]               0\n","          Conv2d-335             [-1, 32, 7, 7]          36,864\n"," BottleneckLayer-336            [-1, 640, 7, 7]               0\n","     BatchNorm2d-337            [-1, 640, 7, 7]           1,280\n","            ReLU-338            [-1, 640, 7, 7]               0\n","          Conv2d-339            [-1, 128, 7, 7]          81,920\n","     BatchNorm2d-340            [-1, 128, 7, 7]             256\n","            ReLU-341            [-1, 128, 7, 7]               0\n","          Conv2d-342             [-1, 32, 7, 7]          36,864\n"," BottleneckLayer-343            [-1, 672, 7, 7]               0\n","     BatchNorm2d-344            [-1, 672, 7, 7]           1,344\n","            ReLU-345            [-1, 672, 7, 7]               0\n","          Conv2d-346            [-1, 128, 7, 7]          86,016\n","     BatchNorm2d-347            [-1, 128, 7, 7]             256\n","            ReLU-348            [-1, 128, 7, 7]               0\n","          Conv2d-349             [-1, 32, 7, 7]          36,864\n"," BottleneckLayer-350            [-1, 704, 7, 7]               0\n","     BatchNorm2d-351            [-1, 704, 7, 7]           1,408\n","            ReLU-352            [-1, 704, 7, 7]               0\n","          Conv2d-353            [-1, 128, 7, 7]          90,112\n","     BatchNorm2d-354            [-1, 128, 7, 7]             256\n","            ReLU-355            [-1, 128, 7, 7]               0\n","          Conv2d-356             [-1, 32, 7, 7]          36,864\n"," BottleneckLayer-357            [-1, 736, 7, 7]               0\n","     BatchNorm2d-358            [-1, 736, 7, 7]           1,472\n","            ReLU-359            [-1, 736, 7, 7]               0\n","          Conv2d-360            [-1, 128, 7, 7]          94,208\n","     BatchNorm2d-361            [-1, 128, 7, 7]             256\n","            ReLU-362            [-1, 128, 7, 7]               0\n","          Conv2d-363             [-1, 32, 7, 7]          36,864\n"," BottleneckLayer-364            [-1, 768, 7, 7]               0\n","     BatchNorm2d-365            [-1, 768, 7, 7]           1,536\n","            ReLU-366            [-1, 768, 7, 7]               0\n","          Conv2d-367            [-1, 128, 7, 7]          98,304\n","     BatchNorm2d-368            [-1, 128, 7, 7]             256\n","            ReLU-369            [-1, 128, 7, 7]               0\n","          Conv2d-370             [-1, 32, 7, 7]          36,864\n"," BottleneckLayer-371            [-1, 800, 7, 7]               0\n","     BatchNorm2d-372            [-1, 800, 7, 7]           1,600\n","            ReLU-373            [-1, 800, 7, 7]               0\n","          Conv2d-374            [-1, 128, 7, 7]         102,400\n","     BatchNorm2d-375            [-1, 128, 7, 7]             256\n","            ReLU-376            [-1, 128, 7, 7]               0\n","          Conv2d-377             [-1, 32, 7, 7]          36,864\n"," BottleneckLayer-378            [-1, 832, 7, 7]               0\n","     BatchNorm2d-379            [-1, 832, 7, 7]           1,664\n","            ReLU-380            [-1, 832, 7, 7]               0\n","          Conv2d-381            [-1, 128, 7, 7]         106,496\n","     BatchNorm2d-382            [-1, 128, 7, 7]             256\n","            ReLU-383            [-1, 128, 7, 7]               0\n","          Conv2d-384             [-1, 32, 7, 7]          36,864\n"," BottleneckLayer-385            [-1, 864, 7, 7]               0\n","     BatchNorm2d-386            [-1, 864, 7, 7]           1,728\n","            ReLU-387            [-1, 864, 7, 7]               0\n","          Conv2d-388            [-1, 128, 7, 7]         110,592\n","     BatchNorm2d-389            [-1, 128, 7, 7]             256\n","            ReLU-390            [-1, 128, 7, 7]               0\n","          Conv2d-391             [-1, 32, 7, 7]          36,864\n"," BottleneckLayer-392            [-1, 896, 7, 7]               0\n","     BatchNorm2d-393            [-1, 896, 7, 7]           1,792\n","            ReLU-394            [-1, 896, 7, 7]               0\n","          Conv2d-395            [-1, 128, 7, 7]         114,688\n","     BatchNorm2d-396            [-1, 128, 7, 7]             256\n","            ReLU-397            [-1, 128, 7, 7]               0\n","          Conv2d-398             [-1, 32, 7, 7]          36,864\n"," BottleneckLayer-399            [-1, 928, 7, 7]               0\n","     BatchNorm2d-400            [-1, 928, 7, 7]           1,856\n","            ReLU-401            [-1, 928, 7, 7]               0\n","          Conv2d-402            [-1, 128, 7, 7]         118,784\n","     BatchNorm2d-403            [-1, 128, 7, 7]             256\n","            ReLU-404            [-1, 128, 7, 7]               0\n","          Conv2d-405             [-1, 32, 7, 7]          36,864\n"," BottleneckLayer-406            [-1, 960, 7, 7]               0\n","     BatchNorm2d-407            [-1, 960, 7, 7]           1,920\n","            ReLU-408            [-1, 960, 7, 7]               0\n","          Conv2d-409            [-1, 128, 7, 7]         122,880\n","     BatchNorm2d-410            [-1, 128, 7, 7]             256\n","            ReLU-411            [-1, 128, 7, 7]               0\n","          Conv2d-412             [-1, 32, 7, 7]          36,864\n"," BottleneckLayer-413            [-1, 992, 7, 7]               0\n","     BatchNorm2d-414            [-1, 992, 7, 7]           1,984\n","            ReLU-415            [-1, 992, 7, 7]               0\n","          Conv2d-416            [-1, 128, 7, 7]         126,976\n","     BatchNorm2d-417            [-1, 128, 7, 7]             256\n","            ReLU-418            [-1, 128, 7, 7]               0\n","          Conv2d-419             [-1, 32, 7, 7]          36,864\n"," BottleneckLayer-420           [-1, 1024, 7, 7]               0\n","          Linear-421                   [-1, 10]          10,250\n","================================================================\n","Total params: 6,961,994\n","Trainable params: 6,961,994\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 359.17\n","Params size (MB): 26.56\n","Estimated Total Size (MB): 386.31\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","metadata":{"id":"fyAg6EY9eRns","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8d6161f5-eeb7-4497-b400-a11f2976dcf0"},"source":["epochs = 40\n","for epoch in range(epochs):\n","  print(f'Epoch {epoch + 1}\\n---------------------------------')\n","  train(device, training_dataloader_cifar10, model_cifar10, nn.CrossEntropyLoss(),\n","        torch.optim.SGD(model_cifar10.parameters(),lr=0.1))\n","  test(device, test_dataloader_cifar10, model_cifar10, nn.CrossEntropyLoss())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1\n","---------------------------------\n",".\n","loss: 2.367886   [    0/50000]\n","....................................................................................................\n","loss: 1.881196   [ 6400/50000]\n","....................................................................................................\n","loss: 1.647252   [12800/50000]\n","....................................................................................................\n","loss: 1.842821   [19200/50000]\n","....................................................................................................\n","loss: 1.748433   [25600/50000]\n","....................................................................................................\n","loss: 1.820175   [32000/50000]\n","....................................................................................................\n","loss: 1.569057   [38400/50000]\n","....................................................................................................\n","loss: 1.612423   [44800/50000]\n",".................................................................................Test Error: \n"," Accuracy: 26.4%, Avg loass: 3.381630\n","\n","Epoch 2\n","---------------------------------\n",".\n","loss: 2.083138   [    0/50000]\n","....................................................................................................\n","loss: 1.441707   [ 6400/50000]\n","....................................................................................................\n","loss: 1.341469   [12800/50000]\n","....................................................................................................\n","loss: 1.562654   [19200/50000]\n","....................................................................................................\n","loss: 1.434190   [25600/50000]\n","....................................................................................................\n","loss: 1.479258   [32000/50000]\n","....................................................................................................\n","loss: 1.304075   [38400/50000]\n","....................................................................................................\n","loss: 1.575960   [44800/50000]\n",".................................................................................Test Error: \n"," Accuracy: 26.1%, Avg loass: 3.309714\n","\n","Epoch 3\n","---------------------------------\n",".\n","loss: 1.782312   [    0/50000]\n","....................................................................................................\n","loss: 1.186750   [ 6400/50000]\n","....................................................................................................\n","loss: 1.199934   [12800/50000]\n","....................................................................................................\n","loss: 1.421319   [19200/50000]\n","....................................................................................................\n","loss: 1.148214   [25600/50000]\n","....................................................................................................\n","loss: 1.302369   [32000/50000]\n","....................................................................................................\n","loss: 1.249785   [38400/50000]\n","....................................................................................................\n","loss: 1.383752   [44800/50000]\n",".................................................................................Test Error: \n"," Accuracy: 31.0%, Avg loass: 3.384490\n","\n","Epoch 4\n","---------------------------------\n",".\n","loss: 1.562544   [    0/50000]\n","....................................................................................................\n","loss: 1.086727   [ 6400/50000]\n","....................................................................................................\n","loss: 0.997876   [12800/50000]\n","....................................................................................................\n","loss: 1.272993   [19200/50000]\n","....................................................................................................\n","loss: 1.059663   [25600/50000]\n","....................................................................................................\n","loss: 1.208129   [32000/50000]\n","....................................................................................................\n","loss: 1.171083   [38400/50000]\n","....................................................................................................\n","loss: 1.328626   [44800/50000]\n",".................................................................................Test Error: \n"," Accuracy: 34.4%, Avg loass: 2.722376\n","\n","Epoch 5\n","---------------------------------\n",".\n","loss: 1.395889   [    0/50000]\n","....................................................................................................\n","loss: 0.995983   [ 6400/50000]\n","....................................................................................................\n","loss: 0.832721   [12800/50000]\n","....................................................................................................\n","loss: 1.026826   [19200/50000]\n","....................................................................................................\n","loss: 1.017503   [25600/50000]\n","....................................................................................................\n","loss: 1.148201   [32000/50000]\n","....................................................................................................\n","loss: 1.010554   [38400/50000]\n","....................................................................................................\n","loss: 1.162665   [44800/50000]\n",".................................................................................Test Error: \n"," Accuracy: 34.6%, Avg loass: 2.664151\n","\n","Epoch 6\n","---------------------------------\n",".\n","loss: 1.237880   [    0/50000]\n","....................................................................................................\n","loss: 0.898654   [ 6400/50000]\n","....................................................................................................\n","loss: 0.740782   [12800/50000]\n","....................................................................................................\n","loss: 1.026291   [19200/50000]\n","....................................................................................................\n","loss: 0.905367   [25600/50000]\n","....................................................................................................\n","loss: 1.090065   [32000/50000]\n","....................................................................................................\n","loss: 0.907509   [38400/50000]\n","....................................................................................................\n","loss: 1.107492   [44800/50000]\n",".................................................................................Test Error: \n"," Accuracy: 38.7%, Avg loass: 2.471777\n","\n","Epoch 7\n","---------------------------------\n",".\n","loss: 1.197431   [    0/50000]\n","....................................................................................................\n","loss: 0.891374   [ 6400/50000]\n","....................................................................................................\n","loss: 0.762297   [12800/50000]\n","....................................................................................................\n","loss: 0.888472   [19200/50000]\n","....................................................................................................\n","loss: 0.890004   [25600/50000]\n","....................................................................................................\n","loss: 1.099791   [32000/50000]\n","....................................................................................................\n","loss: 0.890536   [38400/50000]\n","....................................................................................................\n","loss: 0.998124   [44800/50000]\n",".................................................................................Test Error: \n"," Accuracy: 36.2%, Avg loass: 2.855828\n","\n","Epoch 8\n","---------------------------------\n",".\n","loss: 1.041165   [    0/50000]\n","................................................................."]}]},{"cell_type":"code","metadata":{"id":"AtsXqOD_dv60"},"source":["dest_model_path = basic_path + 'densenet_models/model_cifar10.pth'\n","print(dest_model_path)\n","torch.save(model_cifar10.state_dict(), dest_model_path)"],"execution_count":null,"outputs":[]}]}