{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Compose, Resize, Grayscale\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch import optim\n",
    "from torch.nn import functional\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAlexNet(nn.Module):\n",
    "    def __init__(self, number_of_class=1000):\n",
    "        super(MyAlexNet, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            # Conv 1 - 96 kernels of size 11x11\n",
    "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2),\n",
    "            \n",
    "            # Conv 2\n",
    "            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2),\n",
    "            \n",
    "            # Conv 3\n",
    "            nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Conv 4\n",
    "            nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Conv 5\n",
    "            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5, inplace=True),\n",
    "            nn.Linear(in_features=(6*6*256), out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Dropout(p=0.5, inplace=True),\n",
    "            nn.Linear(in_features=4096, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(in_features=4096, out_features=number_of_class)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.network(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch*len(X)\n",
    "            print(f'loss: {loss:>7f}   [{current:>5d}/{size:>5d}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(device, dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f'Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loass: {test_loss:>8f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_n_test(device, model, loss_fn, optimizer, train_dataloader, test_dataloader, epochs=5):\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch + 1} \\n--------------------------------')\n",
    "        lr_scheduler.step()\n",
    "        train(device, train_dataloader, model, loss_fn, optimizer)\n",
    "        test(device, test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 호출해보기 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder('./data/flower_photos', \n",
    "               transform=Compose([Resize((227, 227)), ToTensor()])\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3670, 1835.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset), len(dataset)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data = torch.utils.data.random_split(dataset, [1835, 1835])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataset.Subset at 0x1e9bd8f0288>,\n",
       " <torch.utils.data.dataset.Subset at 0x1e9bd8f02c8>,\n",
       " 1835,\n",
       " 1835)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data, test_data, len(training_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 128\n",
    "# train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "train_dataloader = DataLoader(training_data)\n",
    "test_dataloader = DataLoader(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1835"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X[N, C, H, W]: torch.Size([1, 3, 227, 227])\n",
      "Shape of y: torch.Size([1]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "for X, y in test_dataloader:\n",
    "    print('Shape of X[N, C, H, W]:', X.shape)\n",
    "    print('Shape of y:', y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyAlexNet(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
       "    (4): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
       "    (8): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=True)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.5, inplace=True)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyAlexNet()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_INIT = 0.01\n",
    "MOMENTUM = 0.9\n",
    "LR_DECAY = 0.0005\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    params=model.parameters(),\n",
    "    lr=LR_INIT,\n",
    "    momentum=MOMENTUM,\n",
    "    weight_decay=LR_DECAY,\n",
    ")\n",
    "loss_fn = functional.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program\\python\\python37\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "d:\\program\\python\\python37\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 6.902305   [    0/ 1835]\n",
      "loss: 2.060395   [  100/ 1835]\n",
      "loss: 2.480644   [  200/ 1835]\n",
      "loss: 0.382271   [  300/ 1835]\n",
      "loss: 4.816805   [  400/ 1835]\n",
      "loss: 10.492311   [  500/ 1835]\n",
      "loss:     nan   [  600/ 1835]\n",
      "loss:     nan   [  700/ 1835]\n",
      "loss:     nan   [  800/ 1835]\n",
      "loss:     nan   [  900/ 1835]\n",
      "loss:     nan   [ 1000/ 1835]\n",
      "loss:     nan   [ 1100/ 1835]\n",
      "loss:     nan   [ 1200/ 1835]\n",
      "loss:     nan   [ 1300/ 1835]\n",
      "loss:     nan   [ 1400/ 1835]\n",
      "loss:     nan   [ 1500/ 1835]\n",
      "loss:     nan   [ 1600/ 1835]\n",
      "loss:     nan   [ 1700/ 1835]\n",
      "loss:     nan   [ 1800/ 1835]\n",
      "Test Error: \n",
      " Accuracy: 17.6%, Avg loass:      nan\n",
      "\n",
      "Epoch 2 \n",
      "--------------------------------\n",
      "loss:     nan   [    0/ 1835]\n",
      "loss:     nan   [  100/ 1835]\n",
      "loss:     nan   [  200/ 1835]\n",
      "loss:     nan   [  300/ 1835]\n",
      "loss:     nan   [  400/ 1835]\n",
      "loss:     nan   [  500/ 1835]\n",
      "loss:     nan   [  600/ 1835]\n",
      "loss:     nan   [  700/ 1835]\n",
      "loss:     nan   [  800/ 1835]\n",
      "loss:     nan   [  900/ 1835]\n",
      "loss:     nan   [ 1000/ 1835]\n",
      "loss:     nan   [ 1100/ 1835]\n",
      "loss:     nan   [ 1200/ 1835]\n",
      "loss:     nan   [ 1300/ 1835]\n",
      "loss:     nan   [ 1400/ 1835]\n",
      "loss:     nan   [ 1500/ 1835]\n",
      "loss:     nan   [ 1600/ 1835]\n",
      "loss:     nan   [ 1700/ 1835]\n",
      "loss:     nan   [ 1800/ 1835]\n",
      "Test Error: \n",
      " Accuracy: 17.6%, Avg loass:      nan\n",
      "\n",
      "Epoch 3 \n",
      "--------------------------------\n",
      "loss:     nan   [    0/ 1835]\n",
      "loss:     nan   [  100/ 1835]\n",
      "loss:     nan   [  200/ 1835]\n",
      "loss:     nan   [  300/ 1835]\n",
      "loss:     nan   [  400/ 1835]\n",
      "loss:     nan   [  500/ 1835]\n",
      "loss:     nan   [  600/ 1835]\n",
      "loss:     nan   [  700/ 1835]\n",
      "loss:     nan   [  800/ 1835]\n",
      "loss:     nan   [  900/ 1835]\n",
      "loss:     nan   [ 1000/ 1835]\n",
      "loss:     nan   [ 1100/ 1835]\n",
      "loss:     nan   [ 1200/ 1835]\n",
      "loss:     nan   [ 1300/ 1835]\n",
      "loss:     nan   [ 1400/ 1835]\n",
      "loss:     nan   [ 1500/ 1835]\n",
      "loss:     nan   [ 1600/ 1835]\n",
      "loss:     nan   [ 1700/ 1835]\n",
      "loss:     nan   [ 1800/ 1835]\n",
      "Test Error: \n",
      " Accuracy: 17.6%, Avg loass:      nan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "train_n_test(device, model, loss_fn, optimizer, train_dataloader, test_dataloader, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 호출해보기 - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original training_data size: 60000\n",
      "subset training_data size: 5001\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root='data', train=True, download=True,\n",
    "    transform=Compose([\n",
    "        Grayscale(num_output_channels=3),\n",
    "        Resize((227, 227)),\n",
    "        ToTensor(),\n",
    "    ])\n",
    ")\n",
    "print('original training_data size:', len(training_data))\n",
    "\n",
    "subset_length = 5000 + 1\n",
    "training_data = torch.utils.data.Subset(training_data, [i for i in range(0, subset_length)])\n",
    "print('subset training_data size:', len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original test_data size: 10000\n",
      "subset training_data size: 5001\n"
     ]
    }
   ],
   "source": [
    "test_data = datasets.MNIST(\n",
    "    root='data', train=False, download=True,\n",
    "    transform=Compose([\n",
    "        Grayscale(num_output_channels=3),\n",
    "        Resize((227, 227)),\n",
    "        ToTensor(),\n",
    "    ])\n",
    ")\n",
    "print('original test_data size:', len(test_data))\n",
    "\n",
    "test_data = torch.utils.data.Subset(test_data, [i for i in range(0, subset_length)])\n",
    "print('subset training_data size:', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data)\n",
    "test_dataloader = DataLoader(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyAlexNet(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
       "    (4): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
       "    (8): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=True)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.5, inplace=True)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = MyAlexNet()\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_INIT = 0.01\n",
    "MOMENTUM = 0.9\n",
    "LR_DECAY = 0.0005\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    params=model.parameters(),\n",
    "    lr=LR_INIT,\n",
    "    momentum=MOMENTUM,\n",
    "    weight_decay=LR_DECAY,\n",
    ")\n",
    "loss_fn = functional.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \n",
      "--------------------------------\n",
      "loss: 6.917681   [    0/ 5001]\n",
      "loss: 6.922639   [  100/ 5001]\n",
      "loss: 6.921749   [  200/ 5001]\n",
      "loss: 6.904881   [  300/ 5001]\n",
      "loss: 6.911610   [  400/ 5001]\n",
      "loss: 6.908261   [  500/ 5001]\n",
      "loss: 6.909052   [  600/ 5001]\n",
      "loss: 6.897280   [  700/ 5001]\n",
      "loss: 6.906073   [  800/ 5001]\n",
      "loss: 6.911418   [  900/ 5001]\n",
      "loss: 6.901933   [ 1000/ 5001]\n",
      "loss: 6.900286   [ 1100/ 5001]\n",
      "loss: 6.923900   [ 1200/ 5001]\n",
      "loss: 6.901096   [ 1300/ 5001]\n",
      "loss: 6.904312   [ 1400/ 5001]\n",
      "loss: 6.911658   [ 1500/ 5001]\n",
      "loss: 6.903975   [ 1600/ 5001]\n",
      "loss: 6.919896   [ 1700/ 5001]\n",
      "loss: 6.909002   [ 1800/ 5001]\n",
      "loss: 6.908118   [ 1900/ 5001]\n",
      "loss: 6.920502   [ 2000/ 5001]\n",
      "loss: 6.904340   [ 2100/ 5001]\n",
      "loss: 6.906506   [ 2200/ 5001]\n",
      "loss: 6.895329   [ 2300/ 5001]\n",
      "loss: 6.902744   [ 2400/ 5001]\n",
      "loss: 6.907590   [ 2500/ 5001]\n",
      "loss: 6.908937   [ 2600/ 5001]\n",
      "loss: 6.909175   [ 2700/ 5001]\n",
      "loss: 6.899239   [ 2800/ 5001]\n",
      "loss: 6.923913   [ 2900/ 5001]\n",
      "loss: 6.907804   [ 3000/ 5001]\n",
      "loss: 6.894495   [ 3100/ 5001]\n",
      "loss: 6.909052   [ 3200/ 5001]\n",
      "loss: 6.908105   [ 3300/ 5001]\n",
      "loss: 6.904240   [ 3400/ 5001]\n",
      "loss: 6.919349   [ 3500/ 5001]\n",
      "loss: 6.907033   [ 3600/ 5001]\n",
      "loss: 6.899262   [ 3700/ 5001]\n",
      "loss: 6.921397   [ 3800/ 5001]\n",
      "loss: 6.921268   [ 3900/ 5001]\n",
      "loss: 6.904822   [ 4000/ 5001]\n",
      "loss: 6.904919   [ 4100/ 5001]\n",
      "loss: 6.905263   [ 4200/ 5001]\n",
      "loss: 6.921175   [ 4300/ 5001]\n",
      "loss: 6.906005   [ 4400/ 5001]\n",
      "loss: 6.905316   [ 4500/ 5001]\n",
      "loss: 6.920645   [ 4600/ 5001]\n",
      "loss: 6.905781   [ 4700/ 5001]\n",
      "loss: 6.898310   [ 4800/ 5001]\n",
      "loss: 6.918099   [ 4900/ 5001]\n",
      "loss: 6.905930   [ 5000/ 5001]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loass: 6.907415\n",
      "\n",
      "Epoch 2 \n",
      "--------------------------------\n",
      "loss: 6.917849   [    0/ 5001]\n",
      "loss: 6.920615   [  100/ 5001]\n",
      "loss: 6.922244   [  200/ 5001]\n",
      "loss: 6.904675   [  300/ 5001]\n",
      "loss: 6.910019   [  400/ 5001]\n",
      "loss: 6.905643   [  500/ 5001]\n",
      "loss: 6.907829   [  600/ 5001]\n",
      "loss: 6.896045   [  700/ 5001]\n",
      "loss: 6.906263   [  800/ 5001]\n",
      "loss: 6.906439   [  900/ 5001]\n",
      "loss: 6.902638   [ 1000/ 5001]\n",
      "loss: 6.898396   [ 1100/ 5001]\n",
      "loss: 6.921887   [ 1200/ 5001]\n",
      "loss: 6.900374   [ 1300/ 5001]\n",
      "loss: 6.903656   [ 1400/ 5001]\n",
      "loss: 6.908929   [ 1500/ 5001]\n",
      "loss: 6.902777   [ 1600/ 5001]\n",
      "loss: 6.920482   [ 1700/ 5001]\n",
      "loss: 6.906325   [ 1800/ 5001]\n",
      "loss: 6.907846   [ 1900/ 5001]\n",
      "loss: 6.916658   [ 2000/ 5001]\n",
      "loss: 6.905602   [ 2100/ 5001]\n",
      "loss: 6.909558   [ 2200/ 5001]\n",
      "loss: 6.898025   [ 2300/ 5001]\n",
      "loss: 6.903431   [ 2400/ 5001]\n",
      "loss: 6.904728   [ 2500/ 5001]\n",
      "loss: 6.909820   [ 2600/ 5001]\n",
      "loss: 6.910671   [ 2700/ 5001]\n",
      "loss: 6.896992   [ 2800/ 5001]\n",
      "loss: 6.920421   [ 2900/ 5001]\n",
      "loss: 6.908605   [ 3000/ 5001]\n",
      "loss: 6.899625   [ 3100/ 5001]\n",
      "loss: 6.905665   [ 3200/ 5001]\n",
      "loss: 6.909862   [ 3300/ 5001]\n",
      "loss: 6.906120   [ 3400/ 5001]\n",
      "loss: 6.922221   [ 3500/ 5001]\n",
      "loss: 6.903179   [ 3600/ 5001]\n",
      "loss: 6.898613   [ 3700/ 5001]\n",
      "loss: 6.922894   [ 3800/ 5001]\n",
      "loss: 6.922986   [ 3900/ 5001]\n",
      "loss: 6.904505   [ 4000/ 5001]\n",
      "loss: 6.901483   [ 4100/ 5001]\n",
      "loss: 6.898452   [ 4200/ 5001]\n",
      "loss: 6.916598   [ 4300/ 5001]\n",
      "loss: 6.904742   [ 4400/ 5001]\n",
      "loss: 6.902362   [ 4500/ 5001]\n",
      "loss: 6.918298   [ 4600/ 5001]\n",
      "loss: 6.904510   [ 4700/ 5001]\n",
      "loss: 6.900336   [ 4800/ 5001]\n",
      "loss: 6.917486   [ 4900/ 5001]\n",
      "loss: 6.903897   [ 5000/ 5001]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loass: 6.907415\n",
      "\n",
      "Epoch 3 \n",
      "--------------------------------\n",
      "loss: 6.917784   [    0/ 5001]\n",
      "loss: 6.917708   [  100/ 5001]\n",
      "loss: 6.921782   [  200/ 5001]\n",
      "loss: 6.903932   [  300/ 5001]\n",
      "loss: 6.910849   [  400/ 5001]\n",
      "loss: 6.906679   [  500/ 5001]\n",
      "loss: 6.908096   [  600/ 5001]\n",
      "loss: 6.902522   [  700/ 5001]\n",
      "loss: 6.911148   [  800/ 5001]\n",
      "loss: 6.909135   [  900/ 5001]\n",
      "loss: 6.905976   [ 1000/ 5001]\n",
      "loss: 6.900375   [ 1100/ 5001]\n",
      "loss: 6.926169   [ 1200/ 5001]\n",
      "loss: 6.902369   [ 1300/ 5001]\n",
      "loss: 6.905556   [ 1400/ 5001]\n",
      "loss: 6.908625   [ 1500/ 5001]\n",
      "loss: 6.906630   [ 1600/ 5001]\n",
      "loss: 6.919833   [ 1700/ 5001]\n",
      "loss: 6.911431   [ 1800/ 5001]\n",
      "loss: 6.912103   [ 1900/ 5001]\n",
      "loss: 6.919843   [ 2000/ 5001]\n",
      "loss: 6.903834   [ 2100/ 5001]\n",
      "loss: 6.904524   [ 2200/ 5001]\n",
      "loss: 6.898404   [ 2300/ 5001]\n",
      "loss: 6.905961   [ 2400/ 5001]\n",
      "loss: 6.909290   [ 2500/ 5001]\n",
      "loss: 6.907206   [ 2600/ 5001]\n",
      "loss: 6.910197   [ 2700/ 5001]\n",
      "loss: 6.899562   [ 2800/ 5001]\n",
      "loss: 6.917807   [ 2900/ 5001]\n",
      "loss: 6.906864   [ 3000/ 5001]\n",
      "loss: 6.897588   [ 3100/ 5001]\n",
      "loss: 6.907408   [ 3200/ 5001]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-d9753316d690>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_n_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-89f4c16a9364>\u001b[0m in \u001b[0;36mtrain_n_test\u001b[1;34m(device, model, loss_fn, optimizer, train_dataloader, test_dataloader, epochs)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Epoch {epoch + 1} \\n--------------------------------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-e6bd07569b7e>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(device, dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program\\python\\python37\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    214\u001b[0m                             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m                             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_n_test(device, model2, loss_fn, optimizer, train_dataloader, test_dataloader, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 호출해보기 - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to data\\stl10_binary.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f0a453f1ac4685a28597842c80fa83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2640397119.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting data\\stl10_binary.tar.gz to data\n",
      "original training_data size: 5000\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.STL10(\n",
    "    root='data', split='train', download=True,\n",
    "    transform=Compose([\n",
    "        Resize((227, 227)),\n",
    "        ToTensor(),\n",
    "    ])\n",
    ")\n",
    "print('original training_data size:', len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "original test_data size: 8000\n"
     ]
    }
   ],
   "source": [
    "test_data = datasets.STL10(\n",
    "    root='data', split='test', download=True,\n",
    "    transform=Compose([\n",
    "        Resize((227, 227)),\n",
    "        ToTensor(),\n",
    "    ])\n",
    ")\n",
    "print('original test_data size:', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data)\n",
    "test_dataloader = DataLoader(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = MyAlexNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_INIT = 0.01\n",
    "MOMENTUM = 0.9\n",
    "LR_DECAY = 0.0005\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    params=model.parameters(),\n",
    "    lr=LR_INIT,\n",
    "    momentum=MOMENTUM,\n",
    "    weight_decay=LR_DECAY,\n",
    ")\n",
    "loss_fn = functional.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \n",
      "--------------------------------\n",
      "loss: 6.912450   [    0/ 5000]\n",
      "loss: 6.900013   [  100/ 5000]\n",
      "loss: 6.916574   [  200/ 5000]\n",
      "loss: 6.911217   [  300/ 5000]\n",
      "loss: 6.897245   [  400/ 5000]\n",
      "loss: 6.903224   [  500/ 5000]\n",
      "loss: 6.903133   [  600/ 5000]\n",
      "loss: 6.891865   [  700/ 5000]\n",
      "loss: 6.912898   [  800/ 5000]\n",
      "loss: 6.908477   [  900/ 5000]\n",
      "loss: 6.895142   [ 1000/ 5000]\n",
      "loss: 6.912224   [ 1100/ 5000]\n",
      "loss: 6.912503   [ 1200/ 5000]\n",
      "loss: 6.910120   [ 1300/ 5000]\n",
      "loss: 6.893977   [ 1400/ 5000]\n",
      "loss: 6.908743   [ 1500/ 5000]\n",
      "loss: 6.902219   [ 1600/ 5000]\n",
      "loss: 6.899412   [ 1700/ 5000]\n",
      "loss: 6.912559   [ 1800/ 5000]\n",
      "loss: 6.914780   [ 1900/ 5000]\n",
      "loss: 6.917825   [ 2000/ 5000]\n",
      "loss: 6.914239   [ 2100/ 5000]\n",
      "loss: 6.912888   [ 2200/ 5000]\n",
      "loss: 6.894862   [ 2300/ 5000]\n",
      "loss: 6.895098   [ 2400/ 5000]\n",
      "loss: 6.896302   [ 2500/ 5000]\n",
      "loss: 6.909850   [ 2600/ 5000]\n",
      "loss: 6.911009   [ 2700/ 5000]\n",
      "loss: 6.891070   [ 2800/ 5000]\n",
      "loss: 6.893897   [ 2900/ 5000]\n",
      "loss: 6.897862   [ 3000/ 5000]\n",
      "loss: 6.904000   [ 3100/ 5000]\n",
      "loss: 6.907844   [ 3200/ 5000]\n",
      "loss: 6.912623   [ 3300/ 5000]\n",
      "loss: 6.903701   [ 3400/ 5000]\n",
      "loss: 6.900823   [ 3500/ 5000]\n",
      "loss: 6.912676   [ 3600/ 5000]\n",
      "loss: 6.898391   [ 3700/ 5000]\n",
      "loss: 6.916819   [ 3800/ 5000]\n",
      "loss: 6.898168   [ 3900/ 5000]\n",
      "loss: 6.914261   [ 4000/ 5000]\n",
      "loss: 6.918578   [ 4100/ 5000]\n",
      "loss: 6.911679   [ 4200/ 5000]\n",
      "loss: 6.896848   [ 4300/ 5000]\n",
      "loss: 6.911002   [ 4400/ 5000]\n",
      "loss: 6.909960   [ 4500/ 5000]\n",
      "loss: 6.910010   [ 4600/ 5000]\n",
      "loss: 6.897887   [ 4700/ 5000]\n",
      "loss: 6.916266   [ 4800/ 5000]\n",
      "loss: 6.912067   [ 4900/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loass: 6.904693\n",
      "\n",
      "Epoch 2 \n",
      "--------------------------------\n",
      "loss: 6.913117   [    0/ 5000]\n",
      "loss: 6.897983   [  100/ 5000]\n",
      "loss: 6.911767   [  200/ 5000]\n",
      "loss: 6.915498   [  300/ 5000]\n",
      "loss: 6.895581   [  400/ 5000]\n",
      "loss: 6.905798   [  500/ 5000]\n",
      "loss: 6.904321   [  600/ 5000]\n",
      "loss: 6.893503   [  700/ 5000]\n",
      "loss: 6.907723   [  800/ 5000]\n",
      "loss: 6.907339   [  900/ 5000]\n",
      "loss: 6.896227   [ 1000/ 5000]\n",
      "loss: 6.910882   [ 1100/ 5000]\n",
      "loss: 6.911170   [ 1200/ 5000]\n",
      "loss: 6.911017   [ 1300/ 5000]\n",
      "loss: 6.892134   [ 1400/ 5000]\n",
      "loss: 6.911044   [ 1500/ 5000]\n",
      "loss: 6.901707   [ 1600/ 5000]\n",
      "loss: 6.898450   [ 1700/ 5000]\n",
      "loss: 6.911708   [ 1800/ 5000]\n",
      "loss: 6.910887   [ 1900/ 5000]\n",
      "loss: 6.916580   [ 2000/ 5000]\n",
      "loss: 6.916871   [ 2100/ 5000]\n",
      "loss: 6.911588   [ 2200/ 5000]\n",
      "loss: 6.900155   [ 2300/ 5000]\n",
      "loss: 6.895071   [ 2400/ 5000]\n",
      "loss: 6.895236   [ 2500/ 5000]\n",
      "loss: 6.913897   [ 2600/ 5000]\n",
      "loss: 6.915314   [ 2700/ 5000]\n",
      "loss: 6.893565   [ 2800/ 5000]\n",
      "loss: 6.896327   [ 2900/ 5000]\n",
      "loss: 6.895966   [ 3000/ 5000]\n",
      "loss: 6.900238   [ 3100/ 5000]\n",
      "loss: 6.911692   [ 3200/ 5000]\n",
      "loss: 6.911957   [ 3300/ 5000]\n",
      "loss: 6.904104   [ 3400/ 5000]\n",
      "loss: 6.900003   [ 3500/ 5000]\n",
      "loss: 6.913543   [ 3600/ 5000]\n",
      "loss: 6.897252   [ 3700/ 5000]\n",
      "loss: 6.916430   [ 3800/ 5000]\n",
      "loss: 6.897480   [ 3900/ 5000]\n",
      "loss: 6.915321   [ 4000/ 5000]\n",
      "loss: 6.915460   [ 4100/ 5000]\n",
      "loss: 6.910702   [ 4200/ 5000]\n",
      "loss: 6.896834   [ 4300/ 5000]\n",
      "loss: 6.914047   [ 4400/ 5000]\n",
      "loss: 6.910995   [ 4500/ 5000]\n",
      "loss: 6.910078   [ 4600/ 5000]\n",
      "loss: 6.901341   [ 4700/ 5000]\n",
      "loss: 6.911249   [ 4800/ 5000]\n",
      "loss: 6.911044   [ 4900/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loass: 6.904693\n",
      "\n",
      "Epoch 3 \n",
      "--------------------------------\n",
      "loss: 6.907327   [    0/ 5000]\n",
      "loss: 6.896828   [  100/ 5000]\n",
      "loss: 6.916068   [  200/ 5000]\n",
      "loss: 6.915454   [  300/ 5000]\n",
      "loss: 6.895606   [  400/ 5000]\n",
      "loss: 6.904578   [  500/ 5000]\n",
      "loss: 6.906441   [  600/ 5000]\n",
      "loss: 6.892381   [  700/ 5000]\n",
      "loss: 6.911139   [  800/ 5000]\n",
      "loss: 6.903671   [  900/ 5000]\n",
      "loss: 6.892468   [ 1000/ 5000]\n",
      "loss: 6.913761   [ 1100/ 5000]\n",
      "loss: 6.911182   [ 1200/ 5000]\n",
      "loss: 6.910169   [ 1300/ 5000]\n",
      "loss: 6.893495   [ 1400/ 5000]\n",
      "loss: 6.913374   [ 1500/ 5000]\n",
      "loss: 6.904861   [ 1600/ 5000]\n",
      "loss: 6.901381   [ 1700/ 5000]\n",
      "loss: 6.911145   [ 1800/ 5000]\n",
      "loss: 6.914662   [ 1900/ 5000]\n",
      "loss: 6.916211   [ 2000/ 5000]\n",
      "loss: 6.911461   [ 2100/ 5000]\n",
      "loss: 6.915359   [ 2200/ 5000]\n",
      "loss: 6.901687   [ 2300/ 5000]\n",
      "loss: 6.896379   [ 2400/ 5000]\n",
      "loss: 6.898676   [ 2500/ 5000]\n",
      "loss: 6.909499   [ 2600/ 5000]\n",
      "loss: 6.911554   [ 2700/ 5000]\n",
      "loss: 6.893960   [ 2800/ 5000]\n",
      "loss: 6.895737   [ 2900/ 5000]\n",
      "loss: 6.896926   [ 3000/ 5000]\n",
      "loss: 6.903436   [ 3100/ 5000]\n",
      "loss: 6.909098   [ 3200/ 5000]\n",
      "loss: 6.912796   [ 3300/ 5000]\n",
      "loss: 6.903325   [ 3400/ 5000]\n",
      "loss: 6.898905   [ 3500/ 5000]\n",
      "loss: 6.913730   [ 3600/ 5000]\n",
      "loss: 6.896229   [ 3700/ 5000]\n",
      "loss: 6.915797   [ 3800/ 5000]\n",
      "loss: 6.897069   [ 3900/ 5000]\n",
      "loss: 6.911636   [ 4000/ 5000]\n",
      "loss: 6.913785   [ 4100/ 5000]\n",
      "loss: 6.911849   [ 4200/ 5000]\n",
      "loss: 6.891743   [ 4300/ 5000]\n",
      "loss: 6.917110   [ 4400/ 5000]\n",
      "loss: 6.913025   [ 4500/ 5000]\n",
      "loss: 6.907094   [ 4600/ 5000]\n",
      "loss: 6.900091   [ 4700/ 5000]\n",
      "loss: 6.912893   [ 4800/ 5000]\n",
      "loss: 6.911642   [ 4900/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loass: 6.904693\n",
      "\n",
      "Epoch 4 \n",
      "--------------------------------\n",
      "loss: 6.912692   [    0/ 5000]\n",
      "loss: 6.896709   [  100/ 5000]\n",
      "loss: 6.914075   [  200/ 5000]\n",
      "loss: 6.914334   [  300/ 5000]\n",
      "loss: 6.895404   [  400/ 5000]\n",
      "loss: 6.904285   [  500/ 5000]\n",
      "loss: 6.902880   [  600/ 5000]\n",
      "loss: 6.891246   [  700/ 5000]\n",
      "loss: 6.909513   [  800/ 5000]\n",
      "loss: 6.903905   [  900/ 5000]\n",
      "loss: 6.897041   [ 1000/ 5000]\n",
      "loss: 6.911575   [ 1100/ 5000]\n",
      "loss: 6.911134   [ 1200/ 5000]\n",
      "loss: 6.908759   [ 1300/ 5000]\n",
      "loss: 6.895676   [ 1400/ 5000]\n",
      "loss: 6.911632   [ 1500/ 5000]\n",
      "loss: 6.902283   [ 1600/ 5000]\n",
      "loss: 6.899694   [ 1700/ 5000]\n",
      "loss: 6.914148   [ 1800/ 5000]\n",
      "loss: 6.909719   [ 1900/ 5000]\n",
      "loss: 6.913289   [ 2000/ 5000]\n",
      "loss: 6.915380   [ 2100/ 5000]\n",
      "loss: 6.913795   [ 2200/ 5000]\n",
      "loss: 6.902233   [ 2300/ 5000]\n",
      "loss: 6.896524   [ 2400/ 5000]\n",
      "loss: 6.897582   [ 2500/ 5000]\n",
      "loss: 6.911261   [ 2600/ 5000]\n",
      "loss: 6.911516   [ 2700/ 5000]\n",
      "loss: 6.889819   [ 2800/ 5000]\n",
      "loss: 6.889680   [ 2900/ 5000]\n",
      "loss: 6.899299   [ 3000/ 5000]\n",
      "loss: 6.902890   [ 3100/ 5000]\n",
      "loss: 6.913082   [ 3200/ 5000]\n",
      "loss: 6.913627   [ 3300/ 5000]\n",
      "loss: 6.902359   [ 3400/ 5000]\n",
      "loss: 6.895981   [ 3500/ 5000]\n",
      "loss: 6.914003   [ 3600/ 5000]\n",
      "loss: 6.898345   [ 3700/ 5000]\n",
      "loss: 6.915257   [ 3800/ 5000]\n",
      "loss: 6.900054   [ 3900/ 5000]\n",
      "loss: 6.914278   [ 4000/ 5000]\n",
      "loss: 6.912971   [ 4100/ 5000]\n",
      "loss: 6.911626   [ 4200/ 5000]\n",
      "loss: 6.900564   [ 4300/ 5000]\n",
      "loss: 6.914191   [ 4400/ 5000]\n",
      "loss: 6.911157   [ 4500/ 5000]\n",
      "loss: 6.910082   [ 4600/ 5000]\n",
      "loss: 6.899362   [ 4700/ 5000]\n",
      "loss: 6.914137   [ 4800/ 5000]\n",
      "loss: 6.910256   [ 4900/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loass: 6.904693\n",
      "\n",
      "Epoch 5 \n",
      "--------------------------------\n",
      "loss: 6.910296   [    0/ 5000]\n",
      "loss: 6.895994   [  100/ 5000]\n",
      "loss: 6.914213   [  200/ 5000]\n",
      "loss: 6.913071   [  300/ 5000]\n",
      "loss: 6.895112   [  400/ 5000]\n",
      "loss: 6.906186   [  500/ 5000]\n",
      "loss: 6.902190   [  600/ 5000]\n",
      "loss: 6.896997   [  700/ 5000]\n",
      "loss: 6.910707   [  800/ 5000]\n",
      "loss: 6.906799   [  900/ 5000]\n",
      "loss: 6.899474   [ 1000/ 5000]\n",
      "loss: 6.915176   [ 1100/ 5000]\n",
      "loss: 6.912190   [ 1200/ 5000]\n",
      "loss: 6.907602   [ 1300/ 5000]\n",
      "loss: 6.895026   [ 1400/ 5000]\n",
      "loss: 6.908940   [ 1500/ 5000]\n",
      "loss: 6.904793   [ 1600/ 5000]\n",
      "loss: 6.904858   [ 1700/ 5000]\n",
      "loss: 6.912610   [ 1800/ 5000]\n",
      "loss: 6.916017   [ 1900/ 5000]\n",
      "loss: 6.916542   [ 2000/ 5000]\n",
      "loss: 6.912140   [ 2100/ 5000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-2bef33952616>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_n_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-89f4c16a9364>\u001b[0m in \u001b[0;36mtrain_n_test\u001b[1;34m(device, model, loss_fn, optimizer, train_dataloader, test_dataloader, epochs)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Epoch {epoch + 1} \\n--------------------------------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-e6bd07569b7e>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(device, dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-9b20be189304>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 440\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_n_test(device, model3, loss_fn, optimizer, train_dataloader, test_dataloader, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 호출해보기 - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to C:\\Users\\samsung/.cache\\torch\\hub\\checkpoints\\alexnet-owt-7be5be79.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65af3050f5604a4d8ed56a99d25f68bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=244408911.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "model4 = torchvision.models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original training_data size: 60000\n",
      "subset training_data size: 5001\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root='data', train=True, download=True,\n",
    "    transform=Compose([\n",
    "        Grayscale(num_output_channels=3),\n",
    "        Resize((227, 227)),\n",
    "        ToTensor(),\n",
    "    ])\n",
    ")\n",
    "print('original training_data size:', len(training_data))\n",
    "\n",
    "subset_length = 5000 + 1\n",
    "training_data = torch.utils.data.Subset(training_data, [i for i in range(0, subset_length)])\n",
    "print('subset training_data size:', len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original test_data size: 10000\n",
      "subset training_data size: 5001\n"
     ]
    }
   ],
   "source": [
    "test_data = datasets.MNIST(\n",
    "    root='data', train=False, download=True,\n",
    "    transform=Compose([\n",
    "        Grayscale(num_output_channels=3),\n",
    "        Resize((227, 227)),\n",
    "        ToTensor(),\n",
    "    ])\n",
    ")\n",
    "print('original test_data size:', len(test_data))\n",
    "\n",
    "test_data = torch.utils.data.Subset(test_data, [i for i in range(0, subset_length)])\n",
    "print('subset training_data size:', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data)\n",
    "test_dataloader = DataLoader(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_INIT = 0.01\n",
    "MOMENTUM = 0.9\n",
    "LR_DECAY = 0.0005\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    params=model.parameters(),\n",
    "    lr=LR_INIT,\n",
    "    momentum=MOMENTUM,\n",
    "    weight_decay=LR_DECAY,\n",
    ")\n",
    "loss_fn = functional.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program\\python\\python37\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 7.496165   [    0/ 5001]\n",
      "loss: 11.379951   [  100/ 5001]\n",
      "loss: 8.663388   [  200/ 5001]\n",
      "loss: 16.175499   [  300/ 5001]\n",
      "loss: 10.071582   [  400/ 5001]\n",
      "loss: 10.409053   [  500/ 5001]\n",
      "loss: 10.951009   [  600/ 5001]\n",
      "loss: 9.999689   [  700/ 5001]\n",
      "loss: 12.091156   [  800/ 5001]\n",
      "loss: 12.328613   [  900/ 5001]\n",
      "loss: 11.744202   [ 1000/ 5001]\n",
      "loss: 10.857874   [ 1100/ 5001]\n",
      "loss: 9.166437   [ 1200/ 5001]\n",
      "loss: 10.652464   [ 1300/ 5001]\n",
      "loss: 12.268744   [ 1400/ 5001]\n",
      "loss: 12.374990   [ 1500/ 5001]\n",
      "loss: 13.641829   [ 1600/ 5001]\n",
      "loss: 8.571790   [ 1700/ 5001]\n",
      "loss: 12.617880   [ 1800/ 5001]\n",
      "loss: 12.878156   [ 1900/ 5001]\n",
      "loss: 10.088725   [ 2000/ 5001]\n",
      "loss: 12.809055   [ 2100/ 5001]\n",
      "loss: 11.815282   [ 2200/ 5001]\n",
      "loss: 13.016496   [ 2300/ 5001]\n",
      "loss: 14.303643   [ 2400/ 5001]\n",
      "loss: 14.133695   [ 2500/ 5001]\n",
      "loss: 10.448414   [ 2600/ 5001]\n",
      "loss: 11.263763   [ 2700/ 5001]\n",
      "loss: 9.851080   [ 2800/ 5001]\n",
      "loss: 9.761324   [ 2900/ 5001]\n",
      "loss: 13.061285   [ 3000/ 5001]\n",
      "loss: 10.172316   [ 3100/ 5001]\n",
      "loss: 14.536616   [ 3200/ 5001]\n",
      "loss: 11.860052   [ 3300/ 5001]\n",
      "loss: 11.386036   [ 3400/ 5001]\n",
      "loss: 10.526750   [ 3500/ 5001]\n",
      "loss: 17.185284   [ 3600/ 5001]\n",
      "loss: 15.266825   [ 3700/ 5001]\n",
      "loss: 9.267360   [ 3800/ 5001]\n",
      "loss: 7.775037   [ 3900/ 5001]\n",
      "loss: 12.012693   [ 4000/ 5001]\n",
      "loss: 15.022050   [ 4100/ 5001]\n",
      "loss: 11.913417   [ 4200/ 5001]\n",
      "loss: 9.401033   [ 4300/ 5001]\n",
      "loss: 11.313767   [ 4400/ 5001]\n",
      "loss: 13.578031   [ 4500/ 5001]\n",
      "loss: 10.505597   [ 4600/ 5001]\n",
      "loss: 12.056885   [ 4700/ 5001]\n",
      "loss: 14.760935   [ 4800/ 5001]\n",
      "loss: 7.873342   [ 4900/ 5001]\n",
      "loss: 11.050960   [ 5000/ 5001]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loass: 11.623820\n",
      "\n",
      "Epoch 2 \n",
      "--------------------------------\n",
      "loss: 7.858391   [    0/ 5001]\n",
      "loss: 11.103767   [  100/ 5001]\n",
      "loss: 9.978225   [  200/ 5001]\n",
      "loss: 12.736599   [  300/ 5001]\n",
      "loss: 10.623561   [  400/ 5001]\n",
      "loss: 10.159822   [  500/ 5001]\n",
      "loss: 12.508296   [  600/ 5001]\n",
      "loss: 9.425960   [  700/ 5001]\n",
      "loss: 11.842863   [  800/ 5001]\n",
      "loss: 10.946477   [  900/ 5001]\n",
      "loss: 12.092842   [ 1000/ 5001]\n",
      "loss: 13.620872   [ 1100/ 5001]\n",
      "loss: 8.652412   [ 1200/ 5001]\n",
      "loss: 11.630110   [ 1300/ 5001]\n",
      "loss: 10.715364   [ 1400/ 5001]\n",
      "loss: 12.303564   [ 1500/ 5001]\n",
      "loss: 10.915080   [ 1600/ 5001]\n",
      "loss: 9.452065   [ 1700/ 5001]\n",
      "loss: 12.534256   [ 1800/ 5001]\n",
      "loss: 11.100640   [ 1900/ 5001]\n",
      "loss: 9.123628   [ 2000/ 5001]\n",
      "loss: 12.836103   [ 2100/ 5001]\n",
      "loss: 15.198016   [ 2200/ 5001]\n",
      "loss: 15.012395   [ 2300/ 5001]\n",
      "loss: 16.417559   [ 2400/ 5001]\n",
      "loss: 13.696030   [ 2500/ 5001]\n",
      "loss: 13.112106   [ 2600/ 5001]\n",
      "loss: 11.489569   [ 2700/ 5001]\n",
      "loss: 9.425642   [ 2800/ 5001]\n",
      "loss: 9.340009   [ 2900/ 5001]\n",
      "loss: 9.828307   [ 3000/ 5001]\n",
      "loss: 9.808748   [ 3100/ 5001]\n",
      "loss: 13.212923   [ 3200/ 5001]\n",
      "loss: 13.052452   [ 3300/ 5001]\n",
      "loss: 12.632625   [ 3400/ 5001]\n",
      "loss: 11.156769   [ 3500/ 5001]\n",
      "loss: 15.633387   [ 3600/ 5001]\n",
      "loss: 16.638992   [ 3700/ 5001]\n",
      "loss: 8.065009   [ 3800/ 5001]\n",
      "loss: 8.235079   [ 3900/ 5001]\n",
      "loss: 14.981836   [ 4000/ 5001]\n",
      "loss: 17.054497   [ 4100/ 5001]\n",
      "loss: 10.950937   [ 4200/ 5001]\n",
      "loss: 8.793767   [ 4300/ 5001]\n",
      "loss: 14.134450   [ 4400/ 5001]\n",
      "loss: 12.387256   [ 4500/ 5001]\n",
      "loss: 11.449677   [ 4600/ 5001]\n",
      "loss: 13.099612   [ 4700/ 5001]\n",
      "loss: 15.799746   [ 4800/ 5001]\n",
      "loss: 7.953749   [ 4900/ 5001]\n",
      "loss: 12.770828   [ 5000/ 5001]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loass: 11.623820\n",
      "\n",
      "Epoch 3 \n",
      "--------------------------------\n",
      "loss: 9.486846   [    0/ 5001]\n",
      "loss: 9.794873   [  100/ 5001]\n",
      "loss: 9.366494   [  200/ 5001]\n",
      "loss: 12.501809   [  300/ 5001]\n",
      "loss: 10.967394   [  400/ 5001]\n",
      "loss: 11.713268   [  500/ 5001]\n",
      "loss: 11.669413   [  600/ 5001]\n",
      "loss: 11.162140   [  700/ 5001]\n",
      "loss: 11.766753   [  800/ 5001]\n",
      "loss: 11.875855   [  900/ 5001]\n",
      "loss: 12.088291   [ 1000/ 5001]\n",
      "loss: 13.210500   [ 1100/ 5001]\n",
      "loss: 7.863956   [ 1200/ 5001]\n",
      "loss: 10.623667   [ 1300/ 5001]\n",
      "loss: 11.195206   [ 1400/ 5001]\n",
      "loss: 11.992968   [ 1500/ 5001]\n",
      "loss: 13.665824   [ 1600/ 5001]\n",
      "loss: 7.503075   [ 1700/ 5001]\n",
      "loss: 12.794049   [ 1800/ 5001]\n",
      "loss: 11.801242   [ 1900/ 5001]\n",
      "loss: 7.062501   [ 2000/ 5001]\n",
      "loss: 14.446606   [ 2100/ 5001]\n",
      "loss: 14.144276   [ 2200/ 5001]\n",
      "loss: 14.942523   [ 2300/ 5001]\n",
      "loss: 15.343190   [ 2400/ 5001]\n",
      "loss: 13.367676   [ 2500/ 5001]\n",
      "loss: 12.158108   [ 2600/ 5001]\n",
      "loss: 11.802914   [ 2700/ 5001]\n",
      "loss: 9.049164   [ 2800/ 5001]\n",
      "loss: 11.054860   [ 2900/ 5001]\n",
      "loss: 12.105258   [ 3000/ 5001]\n",
      "loss: 9.995817   [ 3100/ 5001]\n",
      "loss: 10.827870   [ 3200/ 5001]\n",
      "loss: 11.487267   [ 3300/ 5001]\n",
      "loss: 10.703191   [ 3400/ 5001]\n",
      "loss: 11.947872   [ 3500/ 5001]\n",
      "loss: 18.997349   [ 3600/ 5001]\n",
      "loss: 13.290248   [ 3700/ 5001]\n",
      "loss: 8.436048   [ 3800/ 5001]\n",
      "loss: 8.847227   [ 3900/ 5001]\n",
      "loss: 13.234941   [ 4000/ 5001]\n",
      "loss: 15.210897   [ 4100/ 5001]\n",
      "loss: 10.958321   [ 4200/ 5001]\n",
      "loss: 9.552206   [ 4300/ 5001]\n",
      "loss: 12.564741   [ 4400/ 5001]\n",
      "loss: 13.067103   [ 4500/ 5001]\n",
      "loss: 12.540295   [ 4600/ 5001]\n",
      "loss: 14.918677   [ 4700/ 5001]\n",
      "loss: 14.134939   [ 4800/ 5001]\n",
      "loss: 6.828180   [ 4900/ 5001]\n",
      "loss: 9.793757   [ 5000/ 5001]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loass: 11.623820\n",
      "\n",
      "Epoch 4 \n",
      "--------------------------------\n",
      "loss: 8.184334   [    0/ 5001]\n",
      "loss: 10.723122   [  100/ 5001]\n",
      "loss: 8.396896   [  200/ 5001]\n",
      "loss: 14.784884   [  300/ 5001]\n",
      "loss: 10.043137   [  400/ 5001]\n",
      "loss: 11.456387   [  500/ 5001]\n",
      "loss: 11.265479   [  600/ 5001]\n",
      "loss: 9.748421   [  700/ 5001]\n",
      "loss: 12.529278   [  800/ 5001]\n",
      "loss: 12.758538   [  900/ 5001]\n",
      "loss: 12.951430   [ 1000/ 5001]\n",
      "loss: 12.151104   [ 1100/ 5001]\n",
      "loss: 9.005217   [ 1200/ 5001]\n",
      "loss: 11.216513   [ 1300/ 5001]\n",
      "loss: 10.756772   [ 1400/ 5001]\n",
      "loss: 11.465182   [ 1500/ 5001]\n",
      "loss: 12.048532   [ 1600/ 5001]\n",
      "loss: 9.416859   [ 1700/ 5001]\n",
      "loss: 9.929825   [ 1800/ 5001]\n",
      "loss: 12.549857   [ 1900/ 5001]\n",
      "loss: 9.606017   [ 2000/ 5001]\n",
      "loss: 14.459091   [ 2100/ 5001]\n",
      "loss: 12.972508   [ 2200/ 5001]\n",
      "loss: 15.582026   [ 2300/ 5001]\n",
      "loss: 13.069818   [ 2400/ 5001]\n",
      "loss: 13.750728   [ 2500/ 5001]\n",
      "loss: 10.175392   [ 2600/ 5001]\n",
      "loss: 11.029433   [ 2700/ 5001]\n",
      "loss: 9.647556   [ 2800/ 5001]\n",
      "loss: 10.006903   [ 2900/ 5001]\n",
      "loss: 15.568459   [ 3000/ 5001]\n",
      "loss: 9.611293   [ 3100/ 5001]\n",
      "loss: 15.297474   [ 3200/ 5001]\n",
      "loss: 13.421652   [ 3300/ 5001]\n",
      "loss: 14.487327   [ 3400/ 5001]\n",
      "loss: 12.570378   [ 3500/ 5001]\n",
      "loss: 15.790860   [ 3600/ 5001]\n",
      "loss: 14.812531   [ 3700/ 5001]\n",
      "loss: 6.468822   [ 3800/ 5001]\n",
      "loss: 8.602503   [ 3900/ 5001]\n",
      "loss: 12.761586   [ 4000/ 5001]\n",
      "loss: 14.928195   [ 4100/ 5001]\n",
      "loss: 10.614599   [ 4200/ 5001]\n",
      "loss: 9.312078   [ 4300/ 5001]\n",
      "loss: 11.005472   [ 4400/ 5001]\n",
      "loss: 13.882429   [ 4500/ 5001]\n",
      "loss: 11.700283   [ 4600/ 5001]\n",
      "loss: 13.809577   [ 4700/ 5001]\n",
      "loss: 16.613653   [ 4800/ 5001]\n",
      "loss: 7.730375   [ 4900/ 5001]\n",
      "loss: 12.697124   [ 5000/ 5001]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loass: 11.623820\n",
      "\n",
      "Epoch 5 \n",
      "--------------------------------\n",
      "loss: 6.832103   [    0/ 5001]\n",
      "loss: 12.493035   [  100/ 5001]\n",
      "loss: 10.045899   [  200/ 5001]\n",
      "loss: 13.797780   [  300/ 5001]\n",
      "loss: 9.953692   [  400/ 5001]\n",
      "loss: 10.560864   [  500/ 5001]\n",
      "loss: 10.872621   [  600/ 5001]\n",
      "loss: 9.857176   [  700/ 5001]\n",
      "loss: 12.545891   [  800/ 5001]\n",
      "loss: 12.671266   [  900/ 5001]\n",
      "loss: 14.199292   [ 1000/ 5001]\n",
      "loss: 14.132740   [ 1100/ 5001]\n",
      "loss: 7.212874   [ 1200/ 5001]\n",
      "loss: 11.918495   [ 1300/ 5001]\n",
      "loss: 9.654430   [ 1400/ 5001]\n",
      "loss: 13.264069   [ 1500/ 5001]\n",
      "loss: 11.906809   [ 1600/ 5001]\n",
      "loss: 8.991179   [ 1700/ 5001]\n",
      "loss: 11.781882   [ 1800/ 5001]\n",
      "loss: 12.024354   [ 1900/ 5001]\n",
      "loss: 7.184363   [ 2000/ 5001]\n",
      "loss: 14.028502   [ 2100/ 5001]\n",
      "loss: 12.127468   [ 2200/ 5001]\n",
      "loss: 15.640478   [ 2300/ 5001]\n",
      "loss: 15.406775   [ 2400/ 5001]\n",
      "loss: 14.859825   [ 2500/ 5001]\n",
      "loss: 11.348705   [ 2600/ 5001]\n",
      "loss: 10.721188   [ 2700/ 5001]\n",
      "loss: 13.245508   [ 2800/ 5001]\n",
      "loss: 10.213959   [ 2900/ 5001]\n",
      "loss: 14.745704   [ 3000/ 5001]\n",
      "loss: 10.202574   [ 3100/ 5001]\n",
      "loss: 10.532543   [ 3200/ 5001]\n",
      "loss: 10.492445   [ 3300/ 5001]\n",
      "loss: 12.072993   [ 3400/ 5001]\n",
      "loss: 11.481611   [ 3500/ 5001]\n",
      "loss: 17.975756   [ 3600/ 5001]\n",
      "loss: 14.410270   [ 3700/ 5001]\n",
      "loss: 8.521478   [ 3800/ 5001]\n",
      "loss: 7.001636   [ 3900/ 5001]\n",
      "loss: 12.798109   [ 4000/ 5001]\n",
      "loss: 14.168206   [ 4100/ 5001]\n",
      "loss: 10.877178   [ 4200/ 5001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 9.671409   [ 4300/ 5001]\n",
      "loss: 11.643952   [ 4400/ 5001]\n",
      "loss: 14.956113   [ 4500/ 5001]\n",
      "loss: 11.528555   [ 4600/ 5001]\n",
      "loss: 12.035089   [ 4700/ 5001]\n",
      "loss: 15.699138   [ 4800/ 5001]\n",
      "loss: 7.553170   [ 4900/ 5001]\n",
      "loss: 12.864435   [ 5000/ 5001]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loass: 11.623820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_n_test(device, model4, loss_fn, optimizer, train_dataloader, test_dataloader, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
